url,title,extracted text
https://www.linkedin.com/blog/engineering/recommendations/building-a-large-scale-recommendation-system-people-you-may-know,Building a Large-Scale Recommendation System: People You May Know,"Building a Large-Scale Recommendation System: People You May Know
February 6, 2024
Co-authors:
              
Co-authored byParag Agrawal, 
              
Co-authored byViral Gupta, 
              
Co-authored byAman Gupta, 
               and 
                Co-authored byAastha Nigam
LinkedIn’s “People You May Know” (PYMK) feature has long been used by our members to form connections with other members and expand their networks. It’s an essential part of how we fulfill our mission of connecting the world’s professionals to make them more productive and successful. There are many reasons why a member might find a connection valuable - from finding a professional mentor, to networking with a future employer, to reestablishing a connection with a peer from school. These various motivations for connection, combined with our more than one billion members, mean that selecting a candidate pool to display for PYMK is a monumental task. Today, PYMK processes hundreds of terabytes of data and hundreds of billions of potential connections daily to recommend other members you may want to connect with.
The main challenge is that it’s impossible to sift through the entire candidate inventory to generate recommendations within a reasonable time frame . Methods such as Negative Sampling, Adaptive Importance Sampling, and Hierarchical Softmax can help scale the training process, but inference (i.e., scoring) remains a challenge with such a large item inventory. This is due not only to the size of the candidate pool, but also a result of the multiple factors we’re optimizing for in creating the final PYMK list, such as the likelihood of you sending an invitation and that invitation being accepted.
In this blog, we cover how we built our large-scale recommendation system and scaled its scoring mechanism over the last two years to handle more than a billion items while still ensuring high relevance and low serving latency in the recommendations shared with members. We discuss the core design behind PYMK: a multi-stage ranking system with clearly defined stages serving several goals and leveraging varied algorithms. Each stage acts as a funnel, reducing the search space for the next stage. The end result is a final recommendation pool that balances multiple candidate sources, the likelihood of a connection bringing value, and fairness (among other parameters) to create the best possible PYMK experience for both inviters and invitees.
Candidate Generation (L0 Ranking)
The main purpose of our L0 Ranking is to select a few thousand candidates from an inventory of billions of items. The purpose is not to rank the most relevant candidates at the top, but rather to ensure that the most relevant candidates are selected. This makes Recall@k the right metric to evaluate this stage.
The stage consists of multiple candidate generation (CG) sources. These range from graph based CG sources that generate candidates by performing random graph walks (e.g., n-hop neighbors) to embedding based retrieval (EBR) sources that generate candidates via similarity scores to simple heuristic sources (e.g., new LinkedIn members in your geographic area).
Light Ranker (L1 Ranking)
The L1 Ranking stage takes the few thousand candidates generated by the L0 Ranking above, calibrates and ranks them against a common objective, and then reduces them to a select few hundred of the most relevant candidates. Because the L0 Ranking has multiple CG sources—which could be generating very diverse candidates (e.g., graph based versus similarity based)—calibration is an important part of this stage to make these diverse candidates comparable. A lightweight model like logistic regression or XGBoost could be used for calibration.
This stage also uses Recall@k as the primary evaluation metric, but k is now in the 500-800 range (In the L0 ranking stage, k was in the 3,000-5,000 range).
Rich Ranker (L2 Ranking)
The goal of this stage is to rank the most relevant candidate and then further reduce the candidate pool based on this ranking. The L2 Ranking stage consists of multiple heavy models that predict the probability and the value of different engagement events (i.e., invitations sent, invitations accepted, etc.). The models are usually deep neural networks consuming the most powerful member-candidate pair features.
High-precision metrics like AUC and Precision@k are used in this stage. Usually, the output scores of the models in this stage are used in subsequent re-ranking or even in the models of other teams, so metrics like ECE (expected calibration error) are also used.
Re-Ranker
This is the final stage, which consists of multiple re-rankers, such as fairness re-rankers to ensure fairness in terms of protected attributes like gender and age, diversity re-rankers to help ensure varied interests and intents are expressed in the recommendations, and re-rankers to avoid outcomes like overrepresenting platform power users.
This stage also employs Bayesian optimization to estimate the most important parameters, since there are multiple objectives we’re optimizing for in PYMK candidate selection. The multiple models predicting different engagement events in the L2 Ranking are linearly combined using weights. These weights are one of the most important parameters of the system and are estimated in the re-ranking stage via Bayesian optimization techniques.
Online Evaluation
As mentioned, each stage is evaluated via its corresponding offline metrics (L0 and L1 Ranking stages via Recall@k, L2 Ranking stage via AUC/Precision@k, Re-Ranking stage via log-likelihood and diversity metrics). To evaluate the entire system, we rely on A/B tests, which also tell us the online performance of our system. Offline metrics rarely match the online performance measures, which could be due to multiple factors:
Due to this discrepancy between offline and online evaluation, we trust online evaluation via A/B tests to judge our PYMK system’s performance. The launch of multi-stage ranking for PYMK delivered some of the biggest improvements in member engagement and retention in the past 6 years and still continues to do so.
Future Work
Such a complex system continues to pose challenges in terms of maintainability and monitoring. Additionally, there’s the question of how tightly each of the stages should be coupled—while tight coupling increases the accuracy of the overall system, it slows down the development speed. Tackling feedback loops in a multi-stage system is another big challenge that hasn’t been systematically explored by ML practitioners yet.
Our teams continue the journey of solving these challenges and adopting new techniques to reach their end goal of building a high quality and highly functional large-scale recommendation system to help members connect with other members and have productive conversations.
Acknowledgements
It takes a lot of talent and dedication to build the AI systems that drive our mission. We are grateful to the following team members for their contributions in building a large scale recommendation system.
PYMK AI: Yafei Wang, Divya Venugopalan, Ankan Saha, Siyuan Gao, Xinpei Ma, Yangzhan Yang, Yong Hu, Ayan Acharya, Yiou Xiao, and John Liu.
Graph Infra: Juan A. Colmenares.
FAIT: Sathiya Keerthi Selvaraj, Haichao Wei, Tie Wang, Xiaobing Xue, Chengming Jiang, Sen Zhou, and QQ Song.
Growth: Netra Malagi, Chiachi Lo, Nishant Satya Lakshmikanth, Jugpreet Singh Talwar, Xukai Wang, Pratham Alag, Caitlin Crump, Amber Jin, Jenny Jiang and Albert Cui.
Special thanks to the leadership: Shipeng Yu, Tim Jurka, Bobby Nakamoto, Naman Goel, Souvik Ghosh, and Necip Fazil for their instrumental support, and to Benito Leyva, Will Cheng, Jon Adams, Dina To, Katherine Vaiente, Sathiya Keerthi Selvaraj, Yafei Wang, and Chunan Zeng for helping us improve the quality of this post.
Related articles
Recommendations
Parag Agrawal
Jun 18, 2024
Generative AI
Juan Pablo Bottaro
Apr 25, 2024
Gonzalo Aniano Porcile, PhD
Mar 21, 2024"
https://www.linkedin.com/blog/engineering/infrastructure/how-linkedin-moved-its-kubernetes-apis-to-a-different-api-group,How LinkedIn moved its Kubernetes APIs to a different API group,"Infrastructure
How LinkedIn moved its Kubernetes APIs to a different API group
Sr. Staff Software Engineer, Compute Infrastructure
June 26, 2024
Co-authors:
              
Co-authored byAhmet Alp Balkan
               and 
                Co-authored byRonak Nathani
LinkedIn has used Kubernetes for many years because of the capability, scalability and customization it provides our engineering teams. Customization is particularly valuable for a platform of our size and complexity, which is why it is helpful that Kubernetes offers API extensibility through custom resource definitions. As a Kubernetes user, you can define your own APIs, in addition to the builtin APIs that come with Kubernetes, and each custom API belongs to a particular API group. One limitation we encountered is that Kubernetes doesn't offer any migration functionality to rename a custom API or move the custom API to another API group; once you choose a name and API group, it's permanent.
We recently migrated one of LinkedIn’s major internal custom Kubernetes APIs to a new API group, while also introducing major changes to the API. To achieve this, we built the migration machinery we needed ourselves. With several hundred microservices at LinkedIn already using the old API to run their applications, it was critical to complete the migration without creating downtime on our website– a goal we achieved.
This article will explain why we moved this API between Kubernetes API groups, the limitations of the API versioning machinery in Kubernetes, and how we created our own solution –a “mirror controller”– to seamlessly migrate to a new API while the old API was actively being used.
Motivation for the API group migration
Our first internal custom Kubernetes API at LinkedIn –named LiDeployment API– is our version of the Kubernetes builtin Deployment API. It runs stateless workloads and natively integrates with our internal service ecosystem around:
After several years of development, the shape of this custom API and controller implementation were in a suboptimal state due to organically accumulated tech debt and accidental complexity resulting from changing requirements. As a result, the LiDeployment API started to look less consistent with Kubernetes API design conventions and principles.
Before we took on this migration, the LiDeployment API ran several hundred internal microservices applications, and we planned to onboard thousands more. We wanted to use this opportunity to move the API group to a new one called “apps.linkedin.com.” The new API group name better fits its purpose and long-term trajectory, while also offering a more idiomatic API design to better satisfy newer use cases and requirements to make engineers at LinkedIn more productive.
Why we didn’t use Kubernetes API conversion
Kubernetes API machinery provides versioning for custom resources, where the object is stored in one version and converted to different versions by conversion webhooks when a request comes. This means that Kubernetes can use different API versions to present the same object in different forms, but there is still a single object in a specific version stored in etcd.
For example, if you store a custom resource at v1beta2, you can still serve the object at its v1beta1 or v1alpha1 version thanks to conversion webhooks. However, this means any new field you add to “v1beta2” has to be added to all older active apiVersions since the backwards compatibility requires lossless round-trip conversions from one apiVersion to another back and forth.
In practice, this means API versions in Kubernetes do not allow you to add or remove fields to your API. All versions of your object must be isomorphic. This severely limited our ability to introduce fundamental changes to the LiDeployment API. On top of this, Kubernetes version conversion does not allow moving an API from one group to another; and we wanted to do away with our old API group name as mentioned earlier.
We needed to do what the conversion webhook does between API versions across API groups. Therefore, we built our own API migration machinery to carry out a no-downtime migration where we continued to serve the LiDeployment API on both the old and the new API group.
Migrating APIs by mirroring
Once the design and development of the new LiDeployment API (under the apps.linkedin.com apiGroup) and its controller implementation were complete, the next step was to migrate the objects on the old API group to this new API.
Migration plan
Since we had several hundred applications deployed and serving production traffic through the old API, this migration needed to be non-disruptive to the application owners. This meant that application owners could continue to use the old LiDeployment API to submit their resource manifests, and it should continue to work. It wasn’t feasible to introduce a company-wide deployment freeze to accomplish this.
Similarly, we had internal systems that read/wrote the old LiDeployment API to do things like monitoring deployment status or autoscaling. Therefore, both the old and new LiDeployment objects needed to be in the cluster for a prolonged time to ensure these integrations worked during the migration to the new API.
We devised a controlled ramp plan to move each LiDeployment from one API group to another in incremental batches. Defining three distinct stages for a LiDeployment helped us to reason which version is the source of truth and which controller is actively operating on the object. These stages are:
Mirror controller
To facilitate the coordination between the old controller and the new controller and synchronize the data (spec and status) between LiDeployments on the old and the new API group, we built a special type of controller that mirrors the old LiDeployment resources to the new API version that is in another apiGroup.
This controller (which only runs in the “Mirroring” state described above) has a few top-level working principles:
Essentially, the mirror controller’s job is to create the illusion that the old API is still functional. This controller does its job by watching both the old/new LiDeployment API as its event source, reconciling the goal state, which is ensuring that there is a new LiDeployment object with the same name/namespace as the old one in the desired shape.
The controller comes up with the desired shape for the “new” API object (its metadata/spec) mostly by looking at the “old” object. Then, the controller either creates or updates the new object. Any status updates on the new object are reflected on the old object by the controller to give the impression that the old object is still functioning.
Mirroring logic
One of the first things the mirror controller does is mark the new object owned by the old object (through ownerReferences). This helps us clean up the new LiDeployment (and its Pods) if the customer was using the old LiDeployment, and chooses to delete it.
Then, the mirror controller comes up with the values on the new LiDeployment object by copying fields from the old object to the new. Once the new LiDeployment object is created by the mirror controller, the controller continues to merge the changes from the old object to the new object as customers continue to use the old API to configure their apps.
This merging logic is aware of the intricacies of how each field works (similar to Kubernetes conversion webhooks). For example, any label/annotation we wanted to clean up is filtered out, or any bad default value we did not want to carry over is left out of the new configuration at this stage. The merging logic also is aware of how to merge maps/structs, since unsetting a field that’s defaulted by the webhook may mean the controller can get caught in an infinite reconciliation loop.
As a result, the merging logic largely is aware of both the current values on the new and the old LiDeployment, which looks like this in pseudocode:
It is imperative to get the merging logic right, as there isn’t another chance to fix the migrated configuration after the old API is deleted. Rigorous input validation on the provided object helped us ensure the created new objects are in ideal shape. We also performed defaulting on fields missing values to ensure the objects are “complete.” These API validations relieved the new controller code from having to handle missing values on the objects, as recommended by API conventions.
At this stage, any old LiDeployment that was mirrored now has a corresponding “new” LiDeployment object, even though all customers continue to use the old LiDeployment objects, and any existing tooling that integrates with the old LiDeployment API continues to work.
Switching over to the new API
After mirroring all several hundred LiDeployments to the new API (and validating that the apps are running happily on the new controller), we completed two tasks in parallel:
When the application owner starts deploying their app using the new LiDeployment API, it means that the controller should stop mirroring the spec from the old LiDeployment to the new one, as the new API object is now the source of truth.
To facilitate this, we mark the new LiDeployment with “source-of-truth=true” annotation. If this annotation is present on a new LiDeployment object, the mirror controller disassociates the old and the new LiDeployments (by deleting the previously added ownerReference), and it stops updating the new LiDeployment object based on the old object.
At this stage, the mirror controller has completed its job and the app is fully migrated. We clean up the old LiDeployment object as it no longer has any use.
An example reconciliation logic, including the handoff between “mirroring” to “fully migrated” stage, looks like the following in pseudocode:
While mirroring an application from the old to the new API, we could not predict whether an app would run correctly on the new controller implementation (as its Pod spec has heavily changed). Therefore, Pods created via the old controller and the new controller ran at the same time; this way, application availability wasn’t impacted if the new controller wasn’t creating the Pods with the right configuration. Only when the Pods on the new controller readied up, we cleaned up the underlying workload resource (and therefore the Pods) managed by the old controller.
Challenges
Observability
Throughout the migration, we benefited from having per-object controlled ramp annotations, having a rollback mechanism (in case an app did not work happily on the new controller), and used status/conditions to reflect the mirroring status of a LiDeployment to clearly understand the migration phase the app is in.
Kubectl API name conflicts
A benefit that comes with Kubernetes builtin versioning machinery is that the API owner can specify a “preferred” version for clients like kubectl to work with. In our case, we had two CRDs in different API groups with the same name, and there wasn’t a way to specify a precedence order for which CRD was queried when a user runs kubectl get/edit/delete lideployment; so the user might end up working on the wrong object depending on which API group kubectl picks.
Empirically, there seems to be a deterministic order in kubectl when two identical custom APIs exist, though we did not want to rely on it since the behavior is not guaranteed, and we certainly did not want to implement an Extension APIserver just to make this work. As a workaround, we created an API category (e.g. kubectl get apps) that contained both old/new APIs and made our engineers use this category instead.
Double-deployment of apps
As mentioned earlier, we ran Pods from old/new versions of the controller simultaneously for a brief period of time to validate the Pods created by the new controller with the new spec ran correctly.
This revealed some stateless applications that took a dependency on the assumption that they may have at most one replica, which was not guaranteed by the system.
Conclusion
While the Kubernetes custom resource machinery gives a rather basic way to perform minor versioning changes on an object, it’s still possible to harness the power of controllers to build a custom controller that handles migrating custom resource APIs from one API group to another.
Acknowledgments
Special thanks to Scott Nichols for coming up with the mirroring controller approach and spearheading its implementation. Also, thanks to Kutta Srinivasan and Sudheer Vinukonda for reading drafts of this article.
Topics:
              
                Infrastructure
Related articles
Recommendations
Parag Agrawal
Jun 18, 2024
Talent
Zhujun (Allison) Chen
Jun 6, 2024
Machine Learning
Ruonan Hao
May 24, 2024"
https://danluu.com/seo-spam/,"How bad are search results? Let's compare Google, Bing, Marginalia, Kagi, Mwmbl, and ChatGPT","In The birth & death of search engine optimization, Xe suggests

Here's a fun experiment to try. Take an open source project such as yt-dlp and try to find it from a very generic term like ""youtube downloader"". You won't be able to find it because of all of the content farms that try to rank at the top for that term. Even though yt-dlp is probably actually what you want for a tool to download video from YouTube.

More generally, most tech folks I'm connected to seem to think that Google search results are significantly worse than they were ten years ago (Mastodon poll, Twitter poll, Threads poll). However, there's a sizable group of vocal folks who claim that search results are still great. E.g., a bluesky thought leader who gets high engagement says:

i think the rending of garments about how even google search is terrible now is pretty overblown

I suspect what's going on here is that some people have gotten so used working around bad software that they don't even know they're doing it, reflexively doing the modern equivalent of hitting ctrl+s all the time in editors, or ctrl+a; ctrl+c when composing anything in a text box. Every adept user of the modern web has a bag of tricks they use to get decent results from queries. From having watched quite a few users interact with computers, that doesn't appear to be normal, even among people who are quite competent in various technical fields, e.g., mechanical engineering . However, it could be that people who are complaining about bad search result quality are just hopping on the ""everything sucks"" bandwagon and making totally unsubstantiated comments about search quality.

Since it's fairly easy to try out straightforward, naive, queries, let's try some queries. We'll look at three kinds of queries with five search engines plus ChatGPT and we'll turn off our ad blocker to get the non-expert browsing experience. I once had a computer get owned from browsing to a website with a shady ad, so I hope that doesn't happen here (in that case, I was lucky that I could tell that it happened because the malware was doing so much stuff to my computer that it was impossible to not notice).

One kind of query is a selected set of representative queries a friend of mine used to set up her new computer. My friend is a highly competent engineer outside of tech and wanted help learning ""how to use computers"", so I watched her try to set up a computer and pointed out holes in her mental model of how to interact with websites and software .

The second kind of query is queries for the kinds of things I wanted to know in high school where I couldn't find the answer because everyone I asked (teachers, etc.) gave me obviously incorrect answers and I didn't know how to find the right answer. I was able to get the right answer from various textbooks once I got to college and had access to university libraries, but the questions are simple enough that there's no particular reason a high school student shouldn't be able to understand the answers; it's just an issue of finding the answer, so we'll take a look at how easy these answers are to find. The third kind of query is a local query for information I happened to want to get as I was writing this post.

In grading the queries, there's going to be some subjectivity here because, for example, it's not objectively clear if it's better to have moderately relevant results with no scams or very relevant results mixed interspersed with scams that try to install badware or trick you into giving up your credit card info to pay for something you shouldn't pay for. For the purposes of this post, I'm considering scams to be fairly bad, so in that specific example, I'd rate the moderately relevant results above the very relevant results that have scams mixed in. As with my other posts that have some kind of subjective ranking, there's both a short summary as well as a detailed description of results, so you can rank services yourself, if you like.

In the table below, each column is a query and each row is a search engine or ChatGPT. Results are rated (from worst to best) Terrible, Very Bad, Bad, Ok, Good, and Great, with worse results being more red and better results being more blue.

The queries are:

download youtube videos

ad blocker

download firefox

Why do wider tires have better grip?

Why do they keep making cpu transistors smaller?

vancouver snow forecast winter 2023

YouTube Adblock Firefox Tire CPU Snow Marginalia Ok Good Ok Bad Bad Bad ChatGPT V. Bad Great Good V. Bad V. Bad Bad Mwmbl Bad Bad Bad Bad Bad Bad Kagi Bad V. Bad Great Terrible Bad Terrible Google Terrible V. Bad Bad Bad Bad Terrible Bing Terrible Terrible Great Terrible Ok Terrible

Marginalia does relatively well by sometimes providing decent but not great answers and then providing no answers or very obviously irrelevant answers to the questions it can't answer, with a relatively low rate of scams, lower than any other search engine (although, for these queries, ChatGPT returns zero scams and Marginalia returns some).

Interestingly, Mwmbl lets users directly edit search result rankings. I did this for one query, which would score ""Great"" if it was scored after my edit, but it's easy to do well on a benchmark when you optimize specifically for the benchmark, so Mwmbl's scores are without my edits to the ranking criteria.

One thing I found interesting about the Google results was that, in addition to Google's noted propensity to return recent results, there was a strong propensity to return recent youtube videos. This caused us to get videos that seem quite useless for anybody, except perhaps the maker of the video, who appears to be attempting to get ad revenue from the video. For example, when searching for ""ad blocker"", one of the youtube results was a video where the person rambles for 93 seconds about how you should use an ad blocker and then googles ""ad blocker extension"". They then click on the first result and incorrectly say that ""it's officially from Google"", i.e., the ad blocker is either made by Google or has some kind of official Google seal of approval, because it's the first result. They then ramble for another 40 seconds as they install the ad blocker. After it's installed, they incorrectly state ""this is basically one of the most effective ad blocker [sic] on Google Chrome"". The video has 14k views. For reference, Steve Yegge spent a year making high-effort videos and his most viewed video has 8k views, with a typical view count below 2k. This person who's gaming the algorithm by making low quality videos on topics they know nothing about, who's part of the cottage industry of people making videos taking advantage of Google's algorithm prioritizing recent content regardless of quality, is dominating Steve Yegge's videos because they've found search terms that you can rank for if you put anything up. We'll discuss other Google quirks in more detail below.

ChatGPT does its usual thing and impressively outperforms its more traditional competitors in one case, does an ok job in another case, refuses to really answer the question in another case, and ""hallucinates"" nonsense for a number of queries (as usual for ChatGPT, random perturbations can significantly change the results ). It's common to criticize ChatGPT for its hallucinations and, while I don't think that's unfair, as we noted in this 2015, pre-LLM post on AI, I find this general class of criticism to be overrated in that humans and traditional computer systems make the exact same mistakes.

In this case, search engines return various kinds of hallucinated results. In the snow forecast example, we got deliberately fabricated results, one intended to drive ad revenue through shady ads on a fake forecast site, and another intended to trick the user into thinking that the forecast indicates a cold, snowy, winter (the opposite of the actual forecast), seemingly in order to get the user to sign up for unnecessary snow removal services. Other deliberately fabricated results include a site that's intended to look like an objective review site that's actually a fake site designed to funnel you into installing a specific ad blocker, where the ad blocker they funnel you to appears to be a scammy one that tries to get you to pay for ad blocking and doesn't let you unsubscribe, a fake ""organic"" blog post trying to get you to install a chrome extension that exposes all of your shopping to some service (in many cases, it's not possible to tell if a blog post is a fake or shill post, but in this case, they hosted the fake blog post on the domain for the product and, although it's designed to look like there's an entire blog on the topic, there isn't — it's just this one fake blog post), etc.

There were also many results which don't appear to be deliberately fraudulent and are just run-of-the-mill SEO garbage designed to farm ad clicks. These seem to mostly be pre-LLM sites, so they don't read quite like ChatGPT hallucinations, but they're not fundamentally different. Sometimes the goal of these sites is to get users to click on ads that actually scam the user, and sometimes the goal appears to be to generate clicks to non-scam ads. Search engines also returned many seemingly non-deliberate human hallucinations, where people confidently stated incorrect answers in places where user content is highlighted, like quora, reddit, and stack exchange.

On these queries, even ignoring anything that looks like LLM-generated text, I'd rate the major search engines (Google and Bing) as somewhat worse than ChatGPT in terms of returning various kinds of hallucinated or hallucination-adjacent results. While I don't think concerns about LLM hallucinations are illegitimate, the traditional ecosystem has the problem that the system highly incentivizes putting whatever is most profitable for the software supply chain in front of the user which is, in general, quite different from the best result.

For example, if your app store allows ""you might also like"" recommendations, the most valuable ad slot for apps about gambling addiction management will be gambling apps. Allowing gambling ads on an addiction management app is too blatantly user-hostile for any company deliberately allow today, but of course companies that make gambling apps will try to game the system to break through the filtering and they sometimes succeed. And for web search, I just tried this again on the web and one of the two major search engines returned, as a top result, ad-laden SEO blogspam for addiction management. At the top of the page is a multi-part ad, with the top two links being ""GAMES THAT PAY REAL MONEY"" and ""GAMES THAT PAY REAL CASH"". In general, I was getting localized results (lots of .ca domains since I'm in Canada), so you may get somewhat different results if you try this yourself.

Similarly, if the best result is a good, free, ad blocker like ublock origin, the top ad slot is worth a lot more to a company that makes an ad blocker designed to trick you into paying for a lower quality ad blocker with a nearly-uncancellable subscription, so the scam ad blocker is going to outbid the free ad blocker for the top ad slots. These kinds of companies also have a lot more resources to spend on direct SEO, as well as indirect SEO activities like marketing so, unless search engines mount a more effective effort to combat the profit motive, the top results will go to paid ad blockers even though the paid ad blockers are generally significantly worse for users than free ad blockers. If you talk to people who work on ranking, a lot of the biggest ranking signals are derived from clicks and engagement, but this will only drive users to the best results when users are sophisticated enough to know what the best results are, which they generally aren't. Human raters also rate page quality, but this has the exact same problem.

Many Google employees have told me that ads are actually good because they inform the user about options the user wouldn't have otherwise known about, but anyone who tries browsing without an ad blocker will see ads that are various kinds of misleading, ads that try to trick or entrap the user in various ways, by pretending to be a window, or advertising ""GAMES THAT PAY REAL CASH"" at the top of a page on battling gambling addiction, which has managed to SEO itself to a high ranking on gambling addiction searches. In principle, these problems could be mitigated with enough resources, but we can observe that trillion dollar companies have chosen not to invest enough resources combating SEO, spam, etc., that these kinds of scam ads are rarely seen. Instead, a number of top results are actually ads that direct you to scams.

In their original Page Rank paper, Sergei Brin and Larry Page noted that ad-based search is inherently not incentive aligned with providing good results:

Currently, the predominant business model for commercial search engines is advertising. The goals of the advertising business model do not always correspond to providing quality search to users. For example, in our prototype search engine one of the top results for cellular phone is ""The Effect of Cellular Phone Use Upon Driver Attention"", a study which explains in great detail the distractions and risk associated with conversing on a cell phone while driving. This search result came up first because of its high importance as judged by the PageRank algorithm, an approximation of citation importance on the web [Page, 98]. It is clear that a search engine which was taking money for showing cellular phone ads would have difficulty justifying the page that our system returned to its paying advertisers. For this type of reason and historical experience with other media [Bagdikian 83], we expect that advertising funded search engines will be inherently biased towards the advertisers and away from the needs of the Consumers. Since it is very difficult even for experts to evaluate search engines, search engine bias is particularly insidious. A good example was OpenText, which was reported to be selling companies the right to be listed at the top of the search results for particular queries [Marchiori 97]. This type of bias is much more insidious than advertising, because it is not clear who ""deserves"" to be there, and who is willing to pay money to be listed. This business model resulted in an uproar, and OpenText has ceased to be a viable search engine. But less blatant bias are likely to be tolerated by the market. ... This type of bias is very difficult to detect but could still have a significant effect on the market. Furthermore, advertising income often provides an incentive to provide poor quality search results. For example, we noticed a major search engine would not return a large airline’s homepage when the airline’s name was given as a query. It so happened that the airline had placed an expensive ad, linked to the query that was its name. A better search engine would not have required this ad, and possibly resulted in the loss of the revenue from the airline to the search engine. In general, it could be argued from the consumer point of view that the better the search engine is, the fewer advertisements will be needed for the consumer to find what they want. This of course erodes the advertising supported business model of the existing search engines ... we believe the issue of advertising causes enough mixed incentives that it is crucial to have a competitive search engine that is transparent and in the academic realm.

Of course, Google is now dominated by ads and, despite specifically calling out the insidiousness of user conflating real results with paid results, both Google and Bing have made ads look more and more like real search results, to the point that most users usually won't know that they're clicking on ads and not real search results. By the way, this propensity for users to think that everything is an ""organic"" search result is the reason that, in this post, results are ordered by the order the appear on the page, so if four ads appear above the first organic result, the four ads will be rank 1-4 and the organic result will be ranked 5. I've heard Google employees say that AMP didn't impact search ranking because it ""only"" controlled what results went into the ""carousel"" that appeared above search results, as if inserting a carousel and then a bunch of ads above results, pushing results down below the fold, has no impact on how the user interacts with results. It's also common to see search engines ransoming the top slot for companies, so that companies that don't buy the ad for their own name end up with searches for that company putting their competitors at the top, which is also said to not impact search result ranking, a technically correct claim that's basically meaningless to the median user.

When I tried running the query from the paper, ""cellular phone"" (no quotes) and, the top result was a Google Store link to buy Google's own Pixel 7, with the rest of the top results being various Android phones sold on Amazon. That's followed by the Wikipedia page for Mobile Phone, and then a series of commercial results all trying to sell you phones or SEO-spam trying to get you to click on ads or buy phones via their links (the next 7 results were commercial, with the next result after that being an ad-laden SEO blogspam page for the definition of a cell phone with ads of cell phones on it, followed by 3 more commercial results, followed by another ad-laden definition of a phone). The commercial links seem very low quality, e.g., the top link below the carousel after wikipedia is Best Buy's Canadian mobile phone page. The first two products there are an ad slots for eufy's version of the AirTag. The next result is for a monthly financed iPhone that's tied to Rogers, the next for a monthly financed Samsung phone that's tied to TELUS, then we have Samsung's AirTag, an monthly financed iPhone tied to Freedom Mobile, a monthly financed iPhone tied to Freedom mobile in a different color, a monthly financed iPhone tied to Rogers, a screen protector for the iPhone 13, another Samsung AirTag product, an unlocked iPhone 12, a Samsung wall charger, etc.; it's an extremely low quality result with products that people shouldn't be buying (and, based on the number of reviews, aren't buying — the modal number of reviews of the top products is 0 and the median is 1 or 2 even though there are plenty of things people do actually buy from Best Buy Canada and plenty of products that have lots of reviews). The other commercial results that show up are also generally extremely low quality results. The result that Sergei and Larry suggested was a great top result, ""The Effect of Cellular Phone Use Upon Driver Attention"", is nowhere to be seen, buried beneath an avalanche of commercial results. On the other side of things, Google has also gotten into the action by buying ads that trick users, such as paying for an installer to try to trick users into installing Chrome over Firefox.

Anyway, after looking at the results of our test queries, some questions that come to mind are:

How is Marginalia, a search engine built by a single person, so good?

Can Marginalia or another small search engine displace Google for mainstream users?

Can a collection of small search engines provide better results than Google?

Will Mwmbl's user-curation approach work?

Would a search engine like 1996-Metacrawler, which aggregates results from multiple search engines, ChatGPT, Bard, etc., significantly outperform Google?

The first question could easily be its own post and this post is already 17000 words, so maybe we'll examine it another time. We've previously noted that some individuals can be very productive, but of course the details vary in each case.

On the second question, we looked at a similar question in 2016, both the general version, ""I could reproduce this billion dollar company in a weekend"", as well as specific comments about how open source software would make it trivial to surpass Google any day now, such as

Nowadays, most any technology you need is indeed available in OSS and in state of the art. Allow me to plug meta64.com (my own company) as an example. I am using Lucene to index large numbers of news articles, and provide search into them, by searching a Lucene index generated by simple scraping of RSS-crawled content. I would claim that the Lucene technology is near optimal, and this search approach I'm using is nearly identical to what a Google would need to employ. The only true technology advantage Google has is in the sheer number of servers they can put online, which is prohibitively expensive for us small guys. But from a software standpoint, Google will be overtaken by technologies like mine over the next 10 years I predict.

and

Scaling things is always a challenge but as long as Lucene keeps getting better and better there is going to be a point where Google's advantage becomes irrelevant and we can cluster Lucene nodes and distribute search related computations on top and then use something like Hadoop to implement our own open source ranking algorithms. We're not there yet but technology only gets better over time and the choices we as developers make also matter. Even though Amazon and Google look like unbeatable giants now don't discount what incremental improvements can accomplish over a long stretch of time and in technology it's not even that long a stretch. It wasn't very long ago when Windows was the reigning champion. Where is Windows now?

In that 2016 post, we saw that people who thought that open source solutions were set to surpass Google any day now appeared to have no idea how many hard problems must be solved to make a mainstream competitor to Google, including real-time indexing of rapidly-updated sites, like Twitter, newspapers, etc., as well as table-stakes level NLP, which is extremely non-trivial. Since 2016, these problems have gotten significantly harder as there's more real-time content to index and users expect much better NLP. The number of things people expect out of their search engine has increased as well, making the problem harder still, so it still appears to be quite difficult to displace Google as a mainstream search engine for, say, a billion users.

On the other hand, if you want to make a useful search engine for a small number of users, that seems easier than ever because Google returns worse results than it used to for many queries. In our test queries, we saw a number of queries where many or most top results were filled with SEO garbage, a problem that was significantly worse than it was a decade ago, even before the rise of LLMs and that continues to get worse. I typically use search engines in a way that doesn't run into this, but when I look at what ""normal"" users query or if I try naive queries myself, as I did in this post, most results are quite poor, which didn't used to be true.

Another place Google now falls over for me is when finding non-popular pages. I often find that, when I want to find a web page and I correctly remember the contents of the page, even if I do an exact string search, Google won't return the page. Either the page isn't indexed, or the page is effectively not indexed because it lives in some slow corner of the index that doesn't return in time. In order to find the page, I have to remember some text in a page that links to the page (often many clicks removed from the actual page, not just one, so I'm really remembering a page that links to a page that links to a page that links to a page that links to a page and then using archive.org to traverse the links that are now dead), search for that, and then manually navigate the link graph to get to the page. This basically never happened when I searched for something in 2005 and rarely happened in 2015, but this now happens a large fraction of the time I'm looking for something. Even in 2015, Google wasn't actually comprehensive. Just for example, Google search didn't index every tweet. But, at the time, I found Google search better at searching for tweets than Twitter search and I basically never ran across a tweet I wanted to find that wasn't indexed by Google. But now, most of the tweets I want to find aren't returned by Google search , even when I search for ""[exact string from tweet] site:twitter.com"". In the original Page Rank paper, Sergei and Larry said ""Because humans can only type or speak a finite amount, and as computers continue improving, text indexing will scale even better than it does now."" (and that, while machines can generate an effectively infinite amount of content, just indexing human-generated content seems very useful). Pre-LLM, Google certainly had the resources to index every tweet as well as every human generated utterance on every public website, but they seem to have chosen to devote their resources elsewhere and, relative to its size, the public web appears less indexed than ever, or at least less indexed than it's been since the very early days of web search.

Back when Google returned decent results for simple queries and indexed almost any public page I'd want to find, it would've been very difficult for an independent search engine to return results that I find better than Google's. Marginalia in 2016 would've been nothing more than a curiosity for me since Google would give good-enough results for basically anything where Marginalia returns decent results, and Google would give me the correct result in queries for every obscure page I searched for, something that would be extremely difficult for a small engine. But now that Google effectively doesn't index many pages I want to search for, the relatively small indices that independent search engines have doesn't make them non-starters for me and some of them return less SEO garbage than Google, making them better for my use since I generally don't care about real-time results, don't need fancy NLP (and find that much of it actually makes search results worse for me), don't need shopping integrated into my search results, rarely need image search with understanding of images, etc.

On the question of whether or not a collection of small search engines can provide better results than Google for a lot of users, I don't think this is much of a question because the answer has been a resounding ""yes"" for years. However, many people don't believe this is so. For example, a Google TLM replied to the bluesky thought leader at the top of this post with

Somebody tried argue that if the search space were more competitive, with lots of little providers instead of like three big ones, then somehow it would be *more* resistant to ML-based SEO abuse. And... look, if *google* can't currently keep up with it, how will Little Mr. 5% Market Share do it?

presumably referring to arguments like Hillel Wayne's ""Algorithm Monocultures"", to which our bluesky thought leader replied

like 95% of the time, when someone claims that some small, independent company can do something hard better than the market leader can, it’s just cope. economies of scale work pretty well!

In the past, we looked at some examples where the market leader provides a poor product and various other players, often tiny, provide better products and in a future post, we'll look at how economies of scale and diseconomies of scale interact in various areas for tech but, for this post, suffice it to say that it's clear that despite the common ""econ 101"" cocktail party idea that economies of scale should be the dominant factor for search quality, that doesn't appear to be the case when we look at actual results.

On the question of whether or not Mwmbl's user-curated results can work, I would guess no, or at least not without a lot more moderation. Just browsing to Mwmbl shows the last edit to ranking was by user ""betest"", who added some kind of blogspam as the top entry for ""RSS"". It appears to be possible to revert the change, but there's no easily findable way to report the change or the user as spammy.

On the question of whether or not something like Metacrawler, which aggregated results from multiple search engines, would produce superior results today, that's arguably irrelevant since it would either be impossible to legally run as a commercial service or require prohibitive licensing fees, but it seems plausible that, from a technical standpoint, a modern metacrawler would be fairly good today. Metacrawler quickly became irrelevant because Google returned significantly better results than you would get by aggregating results from other search engines, but it doesn't seem like that's the case today.

Going back to the debate between folks like Xe, who believe that straightforward search queries are inundated with crap, and our thought leader, who believes that ""the rending of garments about how even google search is terrible now is pretty overblown"", it appears that Xe is correct. Although Google doesn't publicly provide the ability to see what was historically returned for queries, many people remember when straightforward queries generally returned good results. One of the reasons Google took off so quickly in the 90s, even among expert users of AltaVista, who'd become very adept at adding all sorts of qualifiers to queries to get good results, was that you didn't have to do that with Google. But we've now come full circle and we need to add qualifiers, restrict our search to specific sites, etc., to get good results from Google on what used to be simple queries. If anything, we've gone well past full circle since the contortions we need to get good results are a lot more involved than they were in the AltaVista days.

If you're looking for work, Freshpaint is hiring a recruiter, Software Engineers, and a Support Engineer. I'm in an investor in the company, so you should take this with the usual grain of salt, but if you're looking to join a fast growing early-stage startup, they seem to have found product-market fit and have been growing extremely quickly (revenue-wise).

Thanks to Laurence Tratt, Heath Borders, Justin Blank, Brian Swetland, Viktor Lofgren (who, BTW, I didn't know before writing this post — I only reached out to him to discuss the Marginalia search results after running the queries), Misha Yagudin, @hpincket@fosstodon.org, Jeremey Kun, and Yossi Kreinin for comments/corrections/discussion

Appendix: Other search engines

DuckDuckGo: in the past, when I've compared DDG to Bing while using an ad blocker, the results have been very similar. I also tried DDG here and, removing the Bing ads, the results aren't as similar as they used to be, but they were still similar enough that it didn't seem worth listing DDG results. I use DDG as my default search engine and I think, like Google, it works fine if you know how to query but, for the kinds of naive queries in this post, it doesn't fare particularly well.

wiby.me: Like Marginalia, this is another search engine made for finding relatively obscure results. I tried four of the above queries on wiby and the results were interesting, in that they were really different than what I got from any other search engine, but wiby didn't return relevant results for the queries I tried.

searchmysite.net: Somewhat relevant results for some queries, but not as relevant as Marginalia. Many fewer scams and ad-laden pages than Google, Bing, and Kagi.

indieweb-search.jamesg.blog: seemed to be having an outage. ""Your request could not be processed due to a server error."" for every query.

Teclis: The search box is still there, but any query results in ""Teclis.com is closed due to bot abuse. Teclis results are still available through Kagi's search results, explicitly through the 'Non-commercial Web' lens and also as an API."". A note on the front page reads ""Teclis results are disabled on the site due to insane amount of bot traffic (99.9% traffic were bots).""

Appendix: queries that return good results

I think that most programmers are likely to be able to get good results to every query, except perhaps the tire width vs. grip query, so here's how I found an ok answer to the tire query:

I tried a youtube search, since a lot of the best car-related content is now youtube. A youtube video whose title claims to answer the question (the video doesn't actually answer the question) has a comment recommending Carroll Smith's book ""Tune To Win"". The comment claims that chapter 1 explains why wider tires have more grip, but I couldn't find an explanation anywhere in the book. Chapter 1 does note that race cars typically run wider tires than passenger cars and that passenger cars are moving towards having wider tires and it make some comments about slip angle that give a sketch of an intuitive reason for why you'd end up with better cornering with a wider contact patch, but I couldn't find a comment that explains differences in braking. Also, the book notes that the primary reason for the wider contact patch is that it (indirectly) allows for more less heat buildup, which then lets you design tires that operate over a narrower temperature range, which allows for softer rubber. That may be true, but it doesn't explain much of the observed behavior one might wonder about.

Tune to Win recommends Kummer's The Unified Theory of Tire and Rubber Friction and Hays and Brooke's (actually Browne, but Smith incorrectly says Brooke) The Physics of Tire Traction. Neither of these really explained what's happening either, but looking for similar books turned up Milliken and Millken's Race Car Vehicle Dynamics, which also didn't really explain why but seemed closer to having an explanation. Looking for books similar to Race Car Vehicle Dynamics turned up Guiggiani's The Science of Vehicle Dynamics, which did get at how to think about and model a number of related factors. The last chapter of Guiggiani's book refers to something called the ""brush model"" (of tires) and searching for ""brush model tire width"" turned up a reference to Pacejka's Tire and Vehicle Dynamics, which does start to explain why wider tires have better grip and what kind of modeling of tire and vehicle dynamics you need to do to explain easily observed tire behavior.

As we've noted, people have different tricks for getting good results so, if you have a better way of getting a good result here, I'd be interested in hearing about it. But note that, basically every time I have a post that notes that something doesn't work, the most common suggestion will be to do something that's commonly suggested that doesn't work, even though the post explicitly notes that the commonly suggested thing doesn't work. For example, the most common comment I receive about this post on filesystem correctness is that you can get around all of this stuff by doing the rename trick, even though the post explicitly notes that this doesn't work, explains why it doesn't work, and references a paper which discusses why it doesn't work. A few years later, I gave an expanded talk on the subject, where I noted that people kept suggesting this thing that doesn't work and the most common comment I get on the talk is that you don't need to bother with all of this stuff because you can just do the rename trick (and no, ext4 having auto_da_alloc doesn't mean that this works since you can only do it if you check that you're on a compatible filesystem which automatically replaces the incorrect code with correct code, at which point it's simpler to just write the correct code). If you have a suggestion for the reason wider tires have better grip or for a search which turns up an explanation, please consider making sure that the explanation is not one of the standard incorrect explanations noted in this post and that the explanation can account for all of the behavior that one must be able to account for if one is explaining this phenomenon.

On how to get good results for other queries, since this post is already 17000 words, I'll leave that for a future post on how expert vs. non-expert computer users interact with computers.

Appendix: summary of query results

For each question, answers are ordered from best to worst, with the metric being my subjective impression of how good the result is. These queries were mostly run in November 2023, although a couple were run in mid-December. When I'm running queries, I very rarely write natural language queries myself. However, normal users often write natural language queries, so I arbitrarily did the ""Tire"" and ""Snow"" queries as natural queries. Continuing with the theme of running simple, naive, queries, we used the free version of ChatGPT for this post, which means the queries were run through ChatGPT 3.5. Ideally, we'd run the full matrix of queries using keyword and natural language queries for each query, run a lot more queries, etc., but this post is already 17000 words (converting to pages of a standard length book, that would be something like 70 pages), so running the full matrix of queries with a few more queries would pretty quickly turn this into a book-length post. For work and for certain kinds of data analysis, I'll sometimes do projects that are that comprehensive or more comprehensive, but here, we can't cover anything resembling a comprehensive set of queries and the best we can do is to just try a handful of queries that seem representative and use our judgment to decide if this matches the kind of behavior we and other people generally see, so I don't think it's worth doing something like 4x the work to cover marginally more ground.

For the search engines, all queries were run in a fresh incognito window with cleared cookies, with the exception of Kagi, which doesn't allow logged-out searches. For Kagi, the queries were done with a fresh account with no custom personalization or filters, although they were done in sequence with the same account, so it's possible some kind of personalized ranking was applied to the later queries based on the clicks in the earlier queries. These queries were done in Vancouver, BC, which seems to have applied some kind of localized ranking on some search engines.

download youtube videos Ideally, the top hit would be yt-dlp or a thin, graphical, wrapper around yt-dlp . Links to youtube-dl or other less frequently updated projects would also be ok. Great results ( yt-dlp as a top hit, maybe with youtube-dl in there somewhere, and no scams): none Good results ( youtube-dl as a top hit, maybe with yt-dlp in there somewhere, and no scams): none Ok results ( youtube-dl as a top hit, maybe with yt-dlp in there somewhere, and fewer scams than other search engines): Marginalia: Top link is for youtube-dl . Most links aren't relevant. Many fewer scams than the big search engines Bad results (has some useful links, but also links to a lot of scams) Mwmbl: Some links to bad sites and scams, but fewer than the big search engines. Also has one indirect link to youtube-dl in the top 10 and one for a GUI for youtube-dl Kagi: Mostly links to scammy sites but does have, a couple pages down, a web.archive.org link to the 2010 version of youtube-dl Very bad results (fails to return any kind of useful result) ChatGPT: basically refuses to answer the question, although you can probably prompt engineer your way to an answer if you don't just naively ask the question you want answered Terrible results (fails to return any kind of useful result and is full of scams: Google: Mostly links to sites that try to scam you or charge you for a worse version of free software. Some links to ad-laden listicles which don't have good suggestions. Zero links to good results. Also links to various youtube videos that are the youtube equivalent of blogspam. Bing: Mostly links to sites that try to scam you or charge you for a worse version of free software. Some links to ad-laden listicles which don't have good suggestions. Arguably zero links to good results (although one could make a case that result #10 is an ok result despite seeming to be malware).

ad blocker Ideally, the top link would be to ublock origin. Failing that, having any link to ublock origin would be good Great results (ublock origin is top result, no scams): ChatGPT: First suggestion is ublock origin Good results (ublock origin is high up, but not the top result; results above ublock origin are either obviously not ad blockers or basically work without payment even if they're not as good as ublock origin; no links that directly try to scam you): none Ok results (ublock origin is in there somewhere, fewer scams than other search engines with not many scams) Marginalia: 3rd and 4th results gets you to ublock origin and 8th result is ublock origin. Nothing that appears to try to scam you directly and ""only"" one link to some kind of SEO ad farm scam (which is much better than the major search engines) Bad results (no links to ublock origin and mostly links to things that paywall good features or ad blockers that deliberately let ads through by default): Mwmbl: Lots of irrelevant links and some links to ghostery. One scam link, so fewer scams than commercial search engines Very bad results (exclusively or almost exclusively link to ad blockers that paywall good features or, by default, deliberately let through ads) Google: lots of links to ad blockers that ""participate in the Acceptable Ads program, where publishers agree to ensure their ads meet certain criteria"" (not mentioned in the text, but explained elsewhere if you look into it, so that the main revenue source for companies that do this is advertisers paying the ""ad blocker"" company to not block their ads, making the ""ad blocker"" not only not an ad blocker, but very much not incentive aligned with users. Some links to things that appear to be scams. Zero links to ublock origin. Also links to various youtube videos that are the youtube equivalent of blogspam. Kagi: similar to Google, but with more scams, though fewer than Bing Terrible results (exclusively or almost exclusively link to ad blockers that paywall good features or, by default, deliberately let through ads and has a significant number of scams): Bing: similar to Google, but with more scams and without youtube videospam

download Firefox Ideally, we'd get links to download firefox with no fake or scam links Great results (links to download firefox; no scams): Bing: links to download Firefox Mwmbl: links to download firefox Kagi: links to download firefox Good: ChatGPT: this is a bit funny to categorize, since these are technically incorrect instructions, but a human should easily be able to decode the instructions and download firefox Ok results (some kind of indirect links to download firefox; no scams): Marginalia: indirect links to download Firefox instructions to get to a firefox download Bad results (links to download firefox, with scams): Google: top links are all legitimate, but the #7 result is a scam that tries to get you to install badware and the #10 result is an ad that appears to be some kind of scam that wants your credit card info.

Why do wider tires have better grip? Ideally, would link to an explanation that clearly explains why and doesn't have an incomplete explanation that can't explain a lot of commonly observed behavior Great / Good / Ok results: none Bad results (no results or a very small number of obviously incorrect results): Mwmbl: one obviously incorrect result and no other results Marginalia: two obviously incorrect results and no other results Very bad results: (a very small number of semi-plausible incorrect results) ChatGPT: standard ChatGPT ""hallucination"" that's probably plausible to a lot of people (it sounds like a lot of incorrect internet comments on the topic, but better written) Terrible results (lots of semi-plausible incorrect results, often on ad farms): Google / Bing / Kagi: incorrect ad-laden results with the usual rate of scammy ads

Why do they keep making cpu transistors smaller? Ideally, would link to an explanation that clearly explains why. The best explanations I've seen are in VLSI textbooks, but I've also seen very good explanations in lecture notes and slides Great results (links to a very good explanation, no scams): none Good results (links to an ok explanation, no scams): none Ok results (links to something you can then search on further and get a good explanation if you're good at searching and doesn't rank bad or misleading explanations above the ok explanation): Bing: top set of links had a partial answer that could easily be turned into links to correct answers via more searching. Also had a lot of irrelevant answers and ad-laden SEO'd garbage Bad results (no results or a small number of obviously irrelevant results or lots of semi-plausible wrong results with an ok result somewhere): Marginalia: no answers Mwmbl: one obviously irrelevant answer Google: 5th link has the right keywords to maybe find the right answer with further searches. Most links have misleading or incorrect partial answers. Lots of links to Quora, which don't answer the question. Also lots of links to other bad SEO'd answers Kagi: 10th link has a fairly direct path to getting the correct answer, if you scroll down far enough on the 10th link. Other links aren't good. Very bad results: ChatGPT: doesn't really answer the question. Asking ChatGPT to explain its answers further causes it ""hallucinate"" incorrect reasons.

vancouver snow forecast winter 2023 I'm not sure what the ideal answer is, but a pretty good one would be to Environment Canada's snow forecast, predicting significantly below normal snow (and above normal temperatures) Great results (links to Environment Canada winter 2023 multi-month snow forecast as top result or something equivalently good): none Good results: none Ok results (links to some kind of semi-plausible winter snow forecast that isn't just made-up garbage to drive ad clicks): none Bad results (no results or obviously irrelevant results): Marginalia: no results ChatGPT: incorrect results, but when I accidentally prepended my question with ""User

"", then it returned a link to the right website (but in a way that would make it quite difficult to navigate to a decent result), so perhaps a slightly different prompt would pseudo-randomly cause a ok result here? Mwmbl: a bunch of obviously irrelevant results Very bad results: none Terrible results (links to deliberately faked forecast results): Bing: mostly irrelevant results. The top seemingly-relevant result is the 5th link, but it appears to be some kind of scam site that fabricates fake weather forecasts and makes money by serving ads on the heavily SEO'd site Kagi: top 4 results are from the scam forecast site that's Bing's 5th link Google: mostly irrelevant results and the #1 result is a fake answer from a local snow removal company that projects significant snow and cold weather in an attempt to get you to unnecessarily buy snow removal service for the year. Other results are SEO'd garbage that's full of ads





Appendix: detailed query results

Download youtube videos

For our first query, we'll search ""download youtube videos"" (Xe's suggested search term, ""youtube downloader"" returns very similar results). The ideal result is yt-dlp or a thin, free, wrapper around yt-dlp . yt-dlp is a fork of youtube-dlc , which is a now defunct fork of youtube-dl , which seems to have very few updates nowadays.. A link to one of these older downloaders also seems ok if they still work.

Google

Some youtube downloader site. Has lots of assurances that the website and the tool are safe because they've been checked by ""Norton SafeWeb"". Interacting with the site at all prompts you to install a browser extension and enable notifications. Trying to download any video gives you a full page pop-over for extension installation for something called CyberShield. There appears to be no way to dismiss the popover without clicking on something to try to install it. After going through the links but then choosing not to install CyberShield, no video downloads. Googling ""cybershield chrome extension"" returns a knowledge card with ""Cyber Shield is a browser extension that claims to be a popup blocker but instead displays advertisements in the browser. When installed, this extension will open new tabs in the browser that display advertisements trying to sell software, push fake software updates, and tech support scams."", so CyberShield appears to be badware. Some youtube downloader site. Interacting with the site causes a pop-up prompting you to download their browser extension. Putting a video URL in causes a pop-up to some scam site but does also cause the video to download, so it seems to be possible to download youtube videos here if you're careful not to engage with the scams the site tries to trick you into interacting with PC Magazine listicle on ways to download videos from youtube. Top recommendations are paying for youtube downloads, VLC (which they note didn't work when they tried it), some $15/yr software, some $26/yr software, ""FlixGrab"", then a warning about how the downloader websites are often scammy and they don't recommend any downloader website. The article has more than one ad per suggestion. Some youtube downloader site with shady pop-overs that try to trick you into clicking on ads before you even interact with the page Some youtube downloader site with pop-ups that try to trick you into clicking on scam ads Some youtube downloader site with pop-ups that try to trick you into clicking on scam ads, e.g., ""Samantha 24, vancouver | I want sex, write to WhatsApp | Close / Continue"". Clicking anything (any button, or anywhere else on the site tries to get you to install something called ""Adblock Ultimate"" ZDNet ZDnet listicle. First suggestion is clipware, which apparently bundles a bunch of malware/adware/junkware with the installer: https://www.reddit.com/r/software/comments/w9o1by/warning_about_clipgrab/. The listicle is full of ads and has an autoplay video [YouTube video] Over 2 minutes of ads followed by a video on how to buy youtube premium (2M views on video) [YouTube video] Video that starts off by asking users to watch the whole video (some monetization thing?). The video tries to funnel you to some kind of software to download videos that costs money [YouTube video] PC Magazine video saying that you probably don't ""have to"" download videos since you can use the share button, and then suggests reading their story (the one in result #3) on how to download videos Some youtube downloader site with scam ads. Interacting with the site at all tries to get you to install ""Adblock Ultimate"" Some youtube downloader site with pop-ups that try to trick you into clicking on scam ads Some youtube downloader site with scam ads

Out of 10 ""normal"" results, we have 9 that, in one way or another, try to get you to install badware or are linked to some other kind of ad scam. One page doesn't do this, but it also doesn't suggest the good, free, option for downloading youtube videos and instead suggests a number of paid solutions. We also had three youtube videos, all of which seem to be the video equivalent of SEO blogspam. Interestingly, we didn't get a lot of ads from Google itself despite that happening the last time I tried turning off my ad blocker to do some Google test queries.

Bing

Some youtube downloader site. This is google (2), which has ads for scam sites [EXPLORE FURTHER ... ""Recommended to you based on what's popular""] Some youtube download site, not one we saw from google. Site has multiple pulsing ads and bills itself as ""50% off"" for Christmas (this search was done in mid-November). Trying to download any video pulls up a fake progress bar with a ""too slow? Try [our program] link"". After a while, a link to download the video appears, but it's a trick, and when you click it, it tries to install ""oWebster Search extension"". Googling ""oWebster Search extension"" indicates that it's badware that hijacks your browser to show ads. Two of the top three hits are how to install the extension and the rest of the top hits are how to remove this badware. Many of the removal links are themselves scams that install other badware. After not installing this badware, clicking the download link again results in a pop-over that tries to get you to install the site's software. If you dismiss the pop-over and click the download link again, you just get the pop-over link again, so this site appears to be a pure scam that doesn't let you download videos [EXPLORE FURTHER]. Interacting with the site pops up fake ads with photos of attractive women who allegedly want to chat with you. Clicking the video download button tries to get you to install a copycat ad blocker that displays extra pop-over ads. The site does seem to actually give you a video download, though [EXPLORE FURTHER] Same as (3) [EXPLORE FURTHER] Same as Google (1) (that NortonSafeWeb youtube downloader site that tries to scam you) [EXPLORE FURTHER] A site that converts videos to MP4. I didn't check to see if the site works or is just a scam as the site doesn't even claim to let you download youtube videos Google (1), again. That NortonSafeWeb youtube downloader site that tries to scam you. [EXPLORE FURTHER] A link to youtube.com (the main page) [EXPLORE FURTHER] Some youtube downloader site with a popover that tries to trick you into clicking on an ad. Closing that reveals 12 more ads. There's a scam ad that's made to look like a youtube downloader button. If you scroll past that, there's a text box and a button for trying to download a youtube video. Entering a valid URL results in an error saying there's no video that URL. Gigantic card that actually has a download button. The download button is fake and just takes you to the site. The site loudly proclaims that the software is not adware, spyware, etc.. Quite a few internet commenters note that their antivirus software tags this software as malware. A lot of comments also indicate that the software doesn't work very well but sometimes works. The site for the software has a an embedded youtube video, which displays ""This video has been removed for violating YouTube's Terms of Service"". Oddly, the download links for mac and Linux are not for this software and in fact don't download anything at all and are installation instructions for youtube-dl ; perhaps this makes sense if the windows version is actually malware. The windows download button takes you to a page that lets you download a windows executable. There's also a link to some kind of ad-laden page that tries to trick you into clicking on ads that look like normal buttons PC magazine listicle An ad for some youtube downloader program that claims ""345,764,132 downloads today""; searching the name of this product on reddit seems to indicate that it's malware Ad for some kind of paid downloader software

That's the end of the first page.

Like Google, no good results and a lot of scams and software that may not be a scam but is some kind of lightweight skin around an open source project that charges you instead of letting you use the software for free.

Marginalia

12-year old answer suggesting youtube-dl, which links to a URL which has been taken down and replaced with ""Due to a ruling of the Hamburg Regional Court, access to this website is blocked."" Some SEO'd article, like you see on normal search engines Leawo YouTube Downloader (I don't know what this is, but a quick search at least doesn't make it immediately obvious that this is some kind of badware, unlike the Google and Bing results) Some SEO'd listicle, like you see on normal search engines Bug report for some random software Some random blogger's recommendation for ""4K Video Downloader"". A quick search seems to indicate that this isn't a scam or badware, but it does lock some features behind a paywall, and is therefore worse than yt-dlp or some free wrapper around yt-dlp A blog post on how to install and use yt-dlp . The blogpost notes that it used to be about youtube-dl , but has been updated to yt-dlp . More software that charges you for something you can get for free, although searching for this software on reddit turns up cracks for it A listicle with bizarrely outdated recommendations, like RealPlayer. The entire blog seems to be full of garbage-quality listicles. A script to download youtube videos for something called ""keyboard maestro"", which seems useful if you already use that software, but seems like a poor solution to this problem if you don't already use this software.

The best results by a large margin. The first link doesn't work, but you can easily get to youtube-dl from the first link. I certainly wouldn't try Leawo YouTube Downloader, but at least it's not so scammy that searching for the name of the project mostly returns results about how the project is some kind of badware or a scam, which is better than we got from Google or Bing. And we do get a recommendation with yt-dlp , with instructions in the results that's just a blog post from someone who wants to help people who are trying to download youtube videos.

Kagi

1. That NortonSafeWeb youtube downloader site. Interacting with the site at all prompts you to install a browser extension and enable notifications. Trying to download any video gives you a full page pop-over for extension installation for something called CyberShield. There appears to be no way to dismiss the popover without clicking on something to try to install it

2. Another link to that NortonSafeWeb youtube downloader site. For some reason, this one is tagged with ""Dec 20, 2003"", apparently indicating that the site is from Dec 20th 2003, although that's quite wrong.

3. Some youtube downloader site. Selecting any video to download pushes you to a site with scam ads.

4. Some youtube downloader site. Interacting with the site at all pops up multiple ads that link to scams and the page wants to enable notifications. A pop-up then appears on top of the ads that says ""Ad removed"" with a link for details. This is a scam link to another ad.

5. Another link to the above site

6-7. Under a subsection titled ""Interesting Finds"", there are links to two github repos. One is for transcribing youtube videos to text and the other is for using Google Takeout to backup photos from google photos or your own youtube channel

8. Some youtube downloader site.

9-13. Under a subsection titled ""Blast from the Past"", 4 irrelevant links and a link to youtube-dl's github page, but the 2010 version at archive.org

14. SEO blogspam for youtube help. Has a link that's allegedly for a ""Greasemonkey script for downloading YouTube videos"", but the link just goes to a page with scammy ads

15. Some software that charges you $5/mo to download videos from youtube

Mwmbl

Some youtube video downloader site, but one that no other search engine returned. There's a huge ad panel that displays ""503 NA - Service Deprecating"". The download link does nothing except for pop up some other ad panes that then disappear, leaving just the 503 ""ad"". $20 software for downloading youtube videos 2016 blog post on how to install and use youtube-dl . Sidebar has two low quality ads which don't appear to be scams and the main body has two ads interspersed, making this extremely low on ads compared to analogous results we've seen from large search engines Some youtube video download site. Has a giant banner claiming that it's ""the only YouTube Downloader that is 100% ad-free and contains no popups."", which is probably not true, but the site does seem to be ad free and not have pop-ups. Download link seems to actually work. Youtube video on how to install and use youtube-dlg (a GUI wrapper for youtube-dl ) on Linux (this query was run from a Mac). Link to what was a 2007 blogpost on how to download youtube videos, which automatically forwards to a 2020 ad-laden SEO blogspam listicle with bad suggestions. Article has two autoplay videos. Archive.org shows that the 2007 blog post had some reasonable options in it for the time, so this wasn't always a bad result. A blog post on a major site that's actually a sponsored post trying to get you to a particular video downloader. Searching for comments on this on reddit indicate that users view the app as a waste of money that doesn't work. The site is also full of scammy and misleading ads for other products. E.g., I tried clicking on an ad that purports to save you money on ""products"". It loaded a fake ""checking your computer"" animation that supposedly checked my computer for compatibility with the extension and then another fake checking animation, after which I got a message saying that my computer is compatible and I'm eligible to save money. All I have to do is install this extension. Closing that window opens a new tab that reads ""Hold up! Do you actually not want automated savings at checkout"" with the options ""Yes, Get Coupons"" and ""No, Don't Save"". Clicking ""No, Don't Save"" is actually an ad that takes you back to a link that tries to get you to install a chrome extension. That ""Norton Safe Web"" youtube downloader site, except that the link is wrong and is to the version of the site that purports to download instagram videos instead of the one that purports to download youtube videos. Link to Google help explaining how you can download youtube videos that you personally uploaded SEO blogspam. It immediately has a pop-over to get you to subscribe to their newsletter. Closing that gives you another pop-over with the options ""Subscribe"" and ""later"". Clicking ""later"" does actually dismiss the 2nd pop-over. After closing the pop-overs, the article has instructions on how to install some software for windows. Searching for reviews of the software returns comments like ""This is a PUP/PUA that can download unwanted applications to your pc or even malicious applications.""

Basically the same as Google or Bing.

ChatGPT

Since ChatGPT expects more conversational queries, we'll use the prompt ""How can I download youtube videos?""

The first attempt, on a Monday at 10:38am PT returned ""Our systems are a bit busy at the moment, please take a break and try again soon."". The second attempt returned an answer saying that one should not download videos without paying for YouTube Premium, but if you want to, you can use third-party apps and websites. Following up with the question ""What are the best third-party apps and websites?"" returned another warning that you shouldn't use third-party apps and websites, followed by the ironic-for-GPT warning,

I don't endorse or provide information on specific third-party apps or websites for downloading YouTube videos. It's essential to use caution and adhere to legal and ethical guidelines when it comes to online content.

ad blocker

For our next query, we'll try ""ad blocker"". We'd like to get ublock origin . Failing that, an ad blocker that, by default, blocks ads. Failing that, something that isn't a scam and also doesn't inject extra ads or its own ads. Although what's best may change at any given moment, comparisons I've seen that don't stack the deck have often seemed to show that ublock origin has the best or among the best performance, and ublock origin is free and blocks ads.

Google

""AdBlock — best ad blocker"". Below the fold, notes ""AdBlock participates in the Acceptable Ads program, so unobtrusive ads are not blocked"", so this doesn't block all ads. Adblock Plus | The world's #1 free ad blocker. Pages notes ""Acceptable Ads are allowed by default to support websites"", so this also does not block all ads by default AdBlock. Page notes that "" Since 2015, we have participated in the Acceptable Ads program, where publishers agree to ensure their ads meet certain criteria. Ads that are deemed non-intrusive are shown by default to AdBlock users"", so this doesn't block all ads ""Adblock Plus - free ad blocker"", same as (2), doesn't block all ads ""AdGuard — World's most advanced adblocker!"" Page tries to sell you on some kind of paid software, ""AdGuard for Mac"". Searching for AdGuard turns up a post from this person looking for an ad blocker that blocks ads injected by AdGuard. It seems that you can download it for free, but then, if you don't subscribe, they give you more ads? ""AdBlock Pro"" on safari store; has in-app purchases. It looks like you have to pay to unlock features like blocking videos [YouTube] ""How youtube is handling the adblock backlash"". 30 second video with 15 second ad before the video. Video has no actual content [YoutTube] ""My thoughts on the youtube adblocker drama"" [YouTube] ""How to Block Ads online in Google Chrome for FREE [2023]""; first comment on video is ""your video doesnt [sic] tell how to stop Youtube adds [sic]"". In the video, a person rambles for a bit and then googles ad blocker extension and then clicks the first link (same as our first link), saying, ""If I can go ahead and go to my first website right here, so it's basically officially from Google .... [after installing, as a payment screen pops up asking you to pay $30 or a monthly or annual fee]"" ""AdBlock for Mobile"" on the App Store. It's rated 3.2* on the iOS store. Lots of reviews indicate that it doesn't really work MalwareBytes ad blocker. A quick search indicates that it doesn't block all ads (unclear if that's deliberate or due to bugs) ""Block ads in Chrome | AdGuard ad blocker"", same as (5) [ad] NordVPN [ad] ""#1 Best Free Ad Blocker (2024) - 100% Free Ad Blocker."" Immediately seems scammy in that it has a fake year (this query was run in mid-November 2023). This is for something called TOTAL Ad Block. Searching for TOTAL Ad Block turns up results indicating that it's a scammy app that doesn't let you unsubscribe and basically tries to steal your money 15 [ad] 100% Free & Easy Download - Automatic Ad Blocker. Actually for Avast browser and not an ad blocker. A quick search show that this browser has a history of being less secure than just running chromium and that it collects an unusually large amount of information from users.

No links to ublock origin. Some links to scams, though not nearly as many as when trying to get a youtube downloader. Lots of links to ad blockers that deliberately only block some ads by default.

Bing

1. [ad] ""Automatic Ad Blocker | 100% Free & Easy Download"". [link is actually to avast secure browser, so an entire browser and not an ad blocker; from a quick search, this appears to be a wrapper around chromium that [has a history of being less secure than just running chromium](https://palant.info/2020/01/13/pwning-avast-secure-browser-for-fun-and-profit/) [which collects an unusually large amount of information from users](https://palant.info/2019/10/28/avast-online-security-and-avast-secure-browser-are-spying-on-you/)].

2. [ad] ""#1 Best Free Ad Blocker (2023) | 100% Free Ad Blocker"". Has a pop-over nag window when you mouse over to the URL bar asking you to install it instead of navigating away. Something called TOTAL ad block. Apparently tries to get to sign up for a subscription [and then makes it very difficult to unsubscribe](https://www.reddit.com/r/Adblock/comments/1412m7l/total_adblock_peoples_experiencesopinions/) (apparently, you can't cancel without a phone call, and when you call and tell them to cancel, they still won't do it unless you threaten to issue a chargeback or block the payment from the bank)

3. [ad] ""Best Ad Blocker (2023) | 100% Free Ad Blocker"". Seems to be a fake review site that reviews various ad blockers; ublock origin is listed as #5 with 3.5 stars. TOTAL ad block is listed as #1 with 5 stars, is the only 5 stars ad blocker, has a banner that shows that it's the ""#1 Free Ad Blocker"", is award winning, etc.

If you then click the link to ublock origin, it takes you to a page that ""shows"" that ublock origin has 0 stars on trustpilot. There are multiple big buttons that say ""click to start blocking ads"" that try to get you to install TOTAL ad block. In the bottom right, in what looks like an ad slot, there's an image that says ""visit site"" for ublock origin. The link doesn't take you to ublock origin and instead takes you a site for [the fake ublock origin](https://www.reddit.com/r/ublock/comments/32mos6/ublock_vs_ublock_origin/).

If you then click the link to ublock origin, it takes you to a page that ""shows"" that ublock origin has 0 stars on trustpilot. There are multiple big buttons that say ""click to start blocking ads"" that try to get you to install TOTAL ad block. In the bottom right, in what looks like an ad slot, there's an image that says ""visit site"" for ublock origin. The link doesn't take you to ublock origin and instead takes you a site for [the fake ublock origin](https://www.reddit.com/r/ublock/comments/32mos6/ublock_vs_ublock_origin/). 4. [ad] ""AVG Free Antivirus 2023 | 100% Free, Secure Download"". This at least doesn't pretend to be an ad blocker of any kind.

5. [Explore content from adblockplus.org] A link to the adblock plus blog.

6. [Explore content from adblockplus.org] A link to a list of adblock plus features.

7. ""Adblock Plus | The world's #1 free ad blocker"".

8-13. Sublinks to various pages on the Adblock Plus site.

We're now three screens down from the result, so the equivalent of the above google results is just a bunch of ads and then links to one website. The note that something is an ad is much more subtle than I've seen on any other site. Given what we know about when users confuse ads with organic search results, it's likely that most users don't realize that the top results are ads and think that the links to scam ad blockers or the fake review site that tries to funnel you into installing a scam ad blocker are organic search results.

Marginalia

""Is ad-blocker software permissible?"" from judaism.stackexchange.com Blogspam for Ghosterty. Ghostery's pricing page notes that you have to pay for ""No Private Sponsored Links"", so it seems like some features are behind a pay wall. Wikipedia says ""Since July 2018, with version 8.2, Ghostery shows advertisements of its own to users"", but it seems like this might be opt-in? https://shouldiblockads.com/. Explains why you might want to block ads. First recommendation is ublock origin ""What’s the best ad blocker for you? - Firefox Add-ons Blog"". First recommendation is ublock origin. Also provides what appears to be accurate information about other ad blockers. Blog post that's a personal account of why someone installed an ad blocker. Opera (browser). Blog post, anti-anti-adblocker polemic. ublock origin. Fairphone forum discussion on whether or not one should install an ad blocker. SEO site blogspam (as in, the site is an SEO optimization site and this is blogspam designed to generate backlinks and funnel traffic to the site).

Probably the best result we've seen so far, in that the third and fourth results suggest ublock origin and the first result is very clearly not an ad blocker. It's unfortunate that the second result is blogspam for Ghostery, but this is still better than we see from Google and Bing.

Mwmbl

A bitly link to a ""thinkpiece"" on ad blocking from a VC thought leader. A link to cryptojackingtest, which forwards to Opera (the browser). A link to ghostery. Another link to ghostery. A link to something called 1blocker, which appears to be a paid ad blocker. Searching for reviews turns up comments like ""I did 1blocker free trial and forgot to cancel so it signed me up for annual for $20 [sic]"" (but comments indicate that the ad blocker does work). Blogspam for Ad Guard. There's a banner ad offering 40% off this ad blocker. An extremely ad-laden site that appears to be in the search results because it contains the text ""ad blocker detected"" if you use an ad blocker (I don't see this text on loading the page, but it's in the page preview on Mwmbl). The first page is literally just ads with a ""read more"" button. Clicking ""read more"" takes you to a different page that's full of ads that also has the cartoon, which is the ""content"". Another site that appears to be in the search results because it contains the text ""ad blocker detected"". Malwarebytes ad blocker, which doesn't appear to work. HN comments for article on youtube ad blocker crackdown. Scrolling to the 41st comment returns a recommendation for ublock origin.

Mwmbl lets users suggest results, so I tried signing up to add ublock origin. Gmail put the sign-up email into my spam folder. After adding ublock origin to the search results, it's now the #1 result for ""ad blocker"" when I search logged out, from an incognito window and all other results are pushed down by one. As mentioned above, the score for Mwmbl is from before I edited the search results and not after.

Kagi

1. ""Adblock Plus | The world's #1 free ad blocker"".

2-11. Sublinks to other pages on the Adblock Plus website.

12. ""AdBlock — best ad blocker"".

13. ""Adblock Plus - free ad blocker"".

14. ""YouTube’s Ad Blocker Crackdown"", a blog post that quotes and links to discussions of people talking about the titular topic.

15-18. Under a section titled ""Interesting Finds"", three articles about youtube's crackdown on ad blockers. One has a full page pop-over trying to get you to install TOTAL Adblock with ""Close"" and ""Open"" buttons. The ""Close"" button does nothing and clicking any link or the open button takes to a page advertising TOTAL adblock. There appears to be no way to dismiss the ad and read the actual article without doing something like going to developer tools and deleting the ad elements. The fourth article is titled ""The FBI now recommends using an ad blocker when searching the web"" and 100% of the above the fold content is the header plus a giant ad. Scrolling down, there are a lot more ads.

19. ""AdBlock"".

20. Another link from the Adblock site, ""Ad Blocker for Chrome - Download and Install AdBlock for Chrome Now!"".

21-25. Under a section titled ""Blast from the Past"", optimal.com ad blocker, a medium article on how to subvert adblock, a blog post from a Mozillan titled ""Why Ad Blockers Work"" that's a response to Ars Technica's ""Why Ad Blocking is devastating to the sites you love"", ""Why You Need a Network-Wide Ad-Blocker (Part 1)"", and ""A Popular Ad Blocker Also Helps the Ad Industry"", subtitled ""Millions of people use the tool Ghostery to block online tracking technology—some may not realize that it feeds data to the ad industry.""

Similar quality to Google and Bing. Maybe halfway in between in terms of the number of links to scams.

ChatGPT

Here, we tried the prompt. How do I install the best ad blocker?

First suggestion is ublock origin. Second suggestion is adblock plus. This seems like the best result by a significant margin.

download firefox

Google

1-6. Links to download firefox.

7. Blogspam for firefox download with ads trying to trick you into installing badware.

8-9. Links to download firefox.

10 [ad] Some kind of shady site that claims to have firefox downloads, but where the downloads take you to other sites that try to get you to sign up for an account where they ask for personal information and your credit card number. Also pops up pop-over with window that does the above if you try to actually download firefox. At least one of the sites is some kind of gambling site, so this site might make money off of referring people to gambling sites?

Mostly good links, but 2 out of the top 10 links are scams. And we didn't have a repeat of this situation I saw in 2017, where Google paid to get ranked above Firefox in a search for Firefox. For search queries where almost every search engine returns a lot of scams, I might rate having 2 out of the top 10 links be scams as ""Ok"" or perhaps even better but, here, where most search engines return no fake or scam links, I'm rating this as ""Bad"". You could make a case for ""Ok"" or ""Good"" here by saying that the vast majority of users will click one of the top links and never get as far as the 7th link, but I think that if Google is confident enough that's the case that they view it as unproblematic that the 7th and 10th links are scams, they should just only serve up the top links.

Bing

1-12. Links to download firefox or closely related links.

13. [ad] Avast browser.

That's the entire first page. Seems pretty good. Nothing that looks like a scam.

Marginalia

1. ""Is it better to download Firefox from the website or use the package manager?"" on the UNIX stackexchange

2-9. Various links related to firefox, but not firefox downloads

10. ""Internet Download Accelerator online help""

Definitely worse than Bing, since none of the links are to download Firefox. Depending on how highly you rate users not getting scammed vs. having the exact right link, this might be better or worse than Google. In this post, this scams are relatively highly weighted, so Marginalia ranks above Google here.

Mwmbl

1-7. Links to download firefox.

8. A link to a tumblr that has nothing to do with firefox. The title of the tumblr is ""Love yourself, download firefox"" (that's the title of the entire blog, not a particular blog post).

9. Link to download firefox nightly.

10. Extremely shady link that allegedly downloads firefox. Attempting to download the shady firefox pops up an ad that tries to trick you downloading Opera. I did not run either the Opera or Firefox binaries to see if they're legitimate.

kagi.com

1-3. Links to download firefox.

4-5. Under a heading titled ""Interesting finds"", a 404'd link to a tweet titled ""What happens if you try to download and install Firefox on Windows"" [which used to note that downloading Firefox on windows results in an OS-level pop-up that recommends Edge instead ""to protect your pc""](https://web.archive.org/web/20220403104257/https://twitter.com/plexus/status/1510568329303445507) and some extremely ad-laden article (though, to its credit, the ads don't seem to be scam ads).

6. Link to download firefox.

7-10. 3 links to download very old versions of firefox, and a blog post about some kind of collaboration between firefox and ebay.

11. Mozilla homepage.

12. Link to download firefox.

Maybe halfway in between Bing and Marginalia. No scams, but a lot of irrelevant links. Unlike some of the larger search engines, these links are almost all to download the wrong version of firefox, e.g., I'm on a Mac and almost all of the links are for windows downloads.

ChatGPT

The prompt ""How do I download firefox?"" returned technically incorrect instructions on how to download firefox. The instructions did start with going to the correct site, at which point I think users are likely to be able to download firefox by looking at the site and ignoring the instructions. Seems vaguely similar to marginalia, in that you can get to a download by clicking some links, but it's not exactly the right result. However, I think users are almost certain to find the correct steps and only likely with Marginalia, so ChatGPT is rated more highly than Marginalia for this query.

Why do wider tires have better grip?

Any explanation that's correct must, a minimum, be consistent with the following:

Assuming a baseline of a moderately wide tire for the wheel size. Scaling both of these to make both wider than the OEM tire (but still running a setup that fits in the car without serious modifications) generally gives better dry braking and better lap times. In wet conditions, wider setups often have better braking distances (though this depends a lot on the specific setup) and better lap times, but also aquaplane at lower speeds. Just increasing the wheel width and using the same tire generally gives you better lap times, within reason. Just increasing the tire width and leaving wheel width fixed generally results in worse lap times.

Why tire pressure changes have the impact that they do (I'm not going to define terms in these bullets; if this text doesn't make sense to you, that's ok). At small slip angles, increasing tire pressure results in increased lateral force. In general, lowering tire pressure increases effective friction coefficient (within reason a semi-reasonable range).



This is one that has a lot of standard incorrect or incomplete answers, including:

Wider tires give you more grip because you get more surface area. Wider tires don't, at reasonable tire pressure, give you significantly more surface area.

Wider tires actually don't give you more grip because friction is surface area times a constant and surface area is mediated by air pressure. It's easily empirically observed that wider tires do, in fact, give you better handling and braking.

Wider tires let you use a softer compound, so the real reason wider tires give you more grip is via the softer compound. This could be part of an explanation, but I've generally seen this cited as the only explanation. However, wider tires give you more grip independent of having a softer compound. You can even observe this with the same tire by mounting the exact same tire on a wider wheel (within reason).

The shape of the contact patch when the tire is wider gives you better lateral grip due to [some mumbo jumbo], e.g., ""tire load sensitivity"" or ""dynamic load"". Ok, perhaps, but what's the mechanism that gives wider tires more grip when braking? And also, please explain the mumbo jumbo. For my goal of understanding why this happens, if you just use some word but don't explain the mechanism, this isn't fundamentally different than saying that wider tires have better grip due to magic. When there's some kind of explanation of the mumbo jumbo, there will often be an explanation that only applies to aspect of increased grip, e.g., the explanation will really only apply to lateral grip and not explain why braking distances are decreased.



Google

1. A ""knowledge card"" that says ""Bigger tires provide a wider contact area that optimizes their performance and traction."", which explains nothing. On clicking the link, it's SEO blogspam with many [incorrect statements, such as ""Are wider tires better for snow traction? Or are narrow tires more reliable in the winter months? The simple answer is narrow tires!](https://mastodon.social/@danluu/111441790762754806) Tires with a smaller section width provide more grip in winter conditions. They place higher surface pressure against the road they are being driven on, enabling its snow and ice traction""

2. [Question dropdown] ""do wider tires give you more grip?"", which correctly says ""On a dry road, wider tires will offer more grip than narrow ones, but the risk of aquaplaning will be higher with wide tires."". On clicking the link, there's no explanation of why, let alone an answer to the question we're asking

3. [Question dropdown] ""Do bigger tires give you better traction?"", which says ""What Difference Does The Wheel Size Make? Larger wheels offer better traction, and because they have more rubber on the tire, this also means a better grip on the road"", which has a nonsensical explanation of why. On clicking the link, the link appears to be talking about wheel diameter and is not only wrong, but actually answering the wrong question.

4. [Question dropdown] ""Why do wider tires have more grip physics?"", which then has some of the standard incorrect explanations.

5. ""Do wider wheels improve handling?"", which says ""Wider wheels and wider tires will also lower your steering friction coefficient"". On clicking the link, there's no explanation of why nor is there an answer to the question we're asking.

6. ""What are the disadvantages of wider tires?"", which says ""Harder Handling & Steering"". On clicking the link, there are multiple incorrect statements and no explanation of why.

7. ""Would wider tires increase friction?"", which says ""Force can be stated as Pressure X Area. For a wide tire, the area is large but the force per unit area is small and vice versa. The force of friction is therefore the same whether the tire is wide or not."". Can't load the page due to a 502 error and the page isn't in archive.org, but this seems fine since the page appears to be wrong

8. ""What is the advantage of 20 inch wheels over 18 inch wheels?"" Answers a different question. On clicking the link, it's low quality SEO blogspam.

9. ""Why do race cars have wide tires?"", which says ""Wider tires provide more resistance to slippery spots or grit on the road. Race tracks have gravel, dust, rubber beads and oil on them in spots that limit traction. By covering a larger width, the tires can handle small problems like that better. Wider tires have improved wear characteristics."". Perhaps technically correct, but fundamentally not the answer and highly misleading at best.

10-49. Other question dropdowns that are wrong. Usually both wrong and answering the wrong question, but sometimes giving a wrong answer to the right question and sometimes giving the right answer to the wrong question. I am just now realizing that clicking question dropdowns give you more question dropdowns.

50. ""Why do wider tires get more grip? : r/cars"". The person asks the question I'm asking, concluding with ""This feels like a really dumb question because wider tires=more grip just seems intuitive, but I don't know the answer."". The top answer is total nonsense ""The smaller surface area has more pressure but the same normal force as a larger surface area. If you distribute the same load across more area, each square inch of tire will have less force it's responsible for holding, and thus is less likely to be overcome by the force from the engine"". The #2 answer is a classic reddit answer, ""Yeah, take your science bs and throw it out the window."". The #3 answer has a vaguely plausible sounding answer to why wider tires have better lateral grip, but it's still misleading. Like many of the answers, the answer emphasizes how wider tires give you better lateral grip and has a lengthy explanation for why this should be the case, but wider tires also give you shorter braking distances and the provided explanation cannot explain why wider tires have shorter braking distances so must be missing a significant part of the puzzle. Anyway, none of the rest of the answers really even attempt to explain why

51-54. Other reddit answers bunched with this one, which also don't answer the question, although one of them links to https://www.brachengineering.com/content/publications/Wheel-Slip-Model-2006-Brach-Engineering.pdf, which has some good content, though it doesn't answer the question.

55. SEO blogspam for someone's youtube video; video doesn't answer the question.

56. Extremely ad-laden site with popovers that try to trick you into clicking on ads, etc.; has text I've seen on other pages that's been copied over to make an SEO ad farm (and the text has answers that are incorrect)

Bing

1. Knowledge card which incorrectly states ""Larger contact patch with the ground.""

2-4. Carousel where none of the links answer the question correctly. (3) from bing is (50) from google search results. (2) isn't wrong, but also doesn't answer the question. (3) is SEO blogspam for someone else's youtube video (same link as google.com 55). The video does not answer the question. (3) and (4) are literally the same link and also don't answer the question

5. ""This is why wider tires equals more grip"". SEO blogspam for someone else's youtube video. The youtube video does not answer the question.

6-10. [EXPLORE FURTHER] results. (6) is blatantly wrong, (7) is the same link as (3) and (4), (8) is (2), SEO blogspam for someone else's youtube video and the video doesn't answer the question, (9) is s SEO blogspam for someone else's youtube video and the video doesn't answer the question, (10) is generic SEO blogspam with lots of incorrect information

11. Same link as (2) and (8), still SEO blogspam for someone else's youtube video and the video doesn't answer the question

12-13 [EXPLORE FURTHER] results. (12) is some kind of SEO ad farm that tries to get you to make ""fake"" ad clicks (there are full screen popovers that, if you click them, cause you to click through some kind of ad to some normal site, giving revenue to whoever set up the ad farm). (13) is the website of the person who made one of the two videos that's a common target for SEO blogspam on this topic. It doesn't answer the question, but at least we have the actual source here.

From skimming further, many of the other links are the same links as above. No link appears to answer the question.

Marginalia

Original query returns zero results. Removing the question mark returns one single result, which is the same as (3) and (4) from bing.

Mwmbl

NYT article titled ""Why Women Pay Higher Interest"". This is the only returned result.

Removing the question mark returns an article about bike tires titled ""Fat Tires During the Winter: What You Need to Know""

Kagi

A knowledge card that incorrectly reads ""wider tire has a greater contact patch with the ground, so can provide traction."" (50) from google Reddit question with many incorrect answers Reddit question with many incorrect answers. Top answer is ""The same reason that pressing your hand on the desk and sliding it takes more effort than doing the same with a finger. More rubber on the road = more friction"". (3) and (4) from bing Youtube video titled ""Do wider tyres give you more grip?"". Clicking the video gives you 1:30 in ads before the video plays. The video is good, but it answers the question in the title of the video and not the question being asked of why this is the case. The first ad appears to be an ad revenue scam. The first link actually takes you to a second link, where any click takes you through some ad's referral link to a product. ""This is why wider tires equals more grip"". SEO blogspam for (6) SEO blogspam for another youtube video SEO blogspam for (6) Quora answer where top answer doesn't answer the question and I can't read all of the answers because I'm not logged in or aren't a premium member or something. Google (56), stolen text from other sites and a site that has popovers that try to trick you into clicking ads Pre-chat GPT nonsense text and a page that's full of ads. Unusually, the few ads that I clicked on seemed to be normal ads and not scams. Blogspam for ad farm that has pop-overs that try to get you to install badware. Page with ChatGPT-sounding nonsense. Has a ""Last updated"" timestamp that's sever-side generated to match the exact moment you navigated to the page. Page tries to trick you into clicking on ads with full-page popover. Ads don't seem to be scams, as far as I can tell. Page which incorrectly states ""In summary, a wider tire does not give better traction, it is the same traction similar to a more narrow tire."". Has some ads that get you to try to install badware.

ChatGPT

Provides a list of ""hallucinated"" reasons. The list of reasons has better grammar than most web search results, but still incorrect. It's not surprising that ChatGPT can't answer this question, since it often falls over on questions that are both easier to reason about and where the training data will contain many copies of the correct answer, e.g., Joss Fong noted that, when her niece asked ChatGPT about gravity, the response was nonsense: ""... That's why a feather floats down slowly but a rock drops quickly — the Earth is pulling them both, but the rock gets pulled harder because it's heavier.""

Overall, no search engine gives correct answers. Marginalia seems to be the best here in that it gives only a couple of links to wrong answers and no links to scams.

Why do they keep making cpu transistors smaller?

I had this question when I was in high school and my AP physics teacher explained to me that it was because making the transistors smaller allowed the CPU to be smaller, which let you make the whole computer smaller. Even at age 14, I could see that this was an absurd answer, not really different than today's ChatGPT hallucinations — at the time, computers tended to be much larger than they are now, and full of huge amounts of empty space, with the CPU taking up basically no space relative to the amount of space in the box and, on top of that, CPUs were actually getting bigger and not smaller as computers were getting smaller. I asked some other people and didn't really get an answer. This was also relatively early on the life of the public web and I wasn't able to find an answer other than something like ""smaller transistors are faster"" or ""smaller = less capacitance"". But why are they faster? And what makes them have less capacitance? Specifically, what about the geometry causes that to scale so that transistors get faster? It's not, in general, obvious that things should get faster if you shrink them, e.g., if you naively linearly shrink a wire, it doesn't appear that it should get faster at all because the cross sectional area is reduced quadratically, increasing resistance per distance quadratically. But length is also reduced linearly, so total resistance is increased linearly. And then capacitance also decreases linearly, so it all cancels out. Anyway, for transistors, it turns out the same kind of straightforward scaling logic shows that they speed up (at back then, transistors were large enough and wire delay was relatively small enough that you got extremely large increases in performance for shrinking transistor). You could explain this to a high school student who's taken physics in a few minutes if you had the right explanation, but I couldn't find an answer to this question until I read a VLSI textbook.

There's now enough content on the web that there must be multiple good explanations out there. Just to check, I used non-naive search terms to find some good results. Let's look at what happens when you use the naive search from above, though.

Google

1. A knowledge card that reads ""Smaller transistors can do more calculations without overheating, which makes them more power efficient."", which isn't exactly wrong but also isn't what I'd consider an answer of why. The article is interesting, but is about another topic and doesn't explain why.

2. [Question dropdown], ""Why are transistors getting smaller?"". Site has an immediate ad pop-over on opening. Site doesn't really answer the question, saying ""Since the first integrated circuit was built in the 1950s, silicon transistors have shrunk following Moore’s law, helping pack more of these devices onto microchips to boost their computing power.""

3. [Question dropdown] ""Why do transistors need to be small?"". Answer is ""The capacitance between two conductors is a function of their physical size: smaller dimensions mean smaller capacitances. And because smaller capacitances mean higher speed as well as lower power, smaller transistors can be run at higher clock frequencies and dissipate less heat while doing so"", which isn't wrong, but the site doesn't explain the scaling that made things faster as transistors got smaller. The page mostly seems concerned about discrete components and note that ""In general, passive components like resistors, capacitors and inductors don’t become much better when you make them smaller: in many ways, they become worse. Miniaturizing these components is therefore done mainly just to be able to squeeze them into a smaller volume, and thereby saving PCB space."", so it's really answering a different question

4. [Question dropdown], ""Why microchips are getting smaller?"". SEO blogspam that doesn't answer the question other than saying stuff like ""smaller is faster""

5. [Question dropdown], ""Why are microprocessors getting smaller?"". Link is to stackexchange. The top answer is that yield is better and cost goes down when chips are smaller, which I consider a non-answer, in that it's also extremely expensive to make things smaller, so what explains why the cost reduction is there? And, also, even if the cost didn't go down, companies would still want smaller transistors for performance reasons, so this misses a major reason and arguably the main reason.

#2 answer actually sort of explains it, ""The reason for this is that as the transistor gate gets smaller, threshold voltage and gate capacitance (required drive current) gets lower."", but is both missing parts of the explanation and doesn't provide the nice, intuitive, physical explanation for why this is the case. Other answers are non-answers like ""The CORE reason why CPUs keep getting smaller is simply that, in computing, smaller is more powerful:"". It's possible to get to a real explanation by searching for these terms 6. ""Why are CPU and GPU manufacturers trying to make ..."". Top answer is the non-answer of ""Smaller transistors are faster and use less power. Small is good."" and since it's quora and I'm not a subscriber, the other answers are obscured by a screen that suggests I start a free trial to ""access this answer and support the author as a Quora+ subscriber"".

7-10. sub-links to other quora answers. Since I'm not a subscriber, by screen real estate, most of the content is ads. None of the content I could read answered the question.

Bing

1. Knowledge card with multiple parts. First parts have some mumbo jumbo, but the last part contains a partial answer. If you click on the last part of the answer, it takes you to a stack exchange question that has more detail on the partial answer. There's enough information in the partial answer to do a search and then find a more complete explanation.

2-4. [people also ask] some answers that are sort of related, but don't directly answer the question

5. Stack exchange answer for a different question.

7-10 [explore further] answers to totally unrelated questions, except for 10, which is extremely ad-laden blogspam to a related question that has a bunch of semi-related text with many ads interspersed between the text.

Kagi

1. ""Why does it take multiple years to develop smaller transistors for CPUs and GPUs?"", on r/askscience. Some ok comments, but they answer a different question.

2-5. Other reddit links that don't answer the question. Some of them are people asking this question, but the answers are wrong. Some of the links answer different questions and have quite good answers to those questions.

6. Stackexchange question that has incorrect and misleading answers.

7. Stackexchange question, but a different question.

8. Quora question. The answers I can read without being a member don't really answer the question.

9. Quora question. The answers I can read without being a member don't really answer the question.

10. Metafilter question from 2006. The first answers are fundamentally wrong, but one of the later answers links to the wikipedia page on MOSFET. Unfortunately, the link is to the now-removed anchor #MOSFET_scaling. There's still a scaling section which has a poor explanation. There's also a link to the page on Dennard Scaling, which is technically correct but has a very poor explanation. However, someone could search for more information using these terms and get correct information.

Marginalia

No results

Mwmbl

A link to a Vox article titled ""Why do artists keep making holiday albums?"". This is the only result.

ChatGPT

Has non-answers like ""increase performance"". Asking ChatGPT to expand on this, with ""Please explain the increased performance."" results in more non-answers as well as fairly misleading answers, such as

Shorter Interconnects: Smaller transistors result in shorter distances between them. Shorter interconnects lead to lower resistance and capacitance, reducing the time it takes for signals to travel between transistors. Faster signal propagation enhances the overall speed and efficiency of the integrated circuit ... The reduced time it takes for signals to travel between transistors, combined with lower power consumption, allows for higher clock frequencies

I could see this seeming plausible to someone with no knowledg"
https://research.google/blog/heuristics-on-the-high-seas-mathematical-optimization-for-cargo-ships/,Heuristics on the high seas: Mathematical optimization for cargo ships,"We present a previously unknown solution to the Liner Shipping Network Design and Scheduling Problem, which is part of our new Shipping Network Design API.

Look around you. Chances are that something in your line of sight sailed on a cargo ship. 90% of the world's goods travel over the ocean, often on cargo vessels mammoth in scale: a quarter mile long, weighing 250,000 tons, holding 12,000 containers of goods collectively worth a billion dollars. Unlike airplanes, trains, and trucks, cargo ships are in nearly constant operation, following cyclical routes across oceans.

A ""post-Panamax"" container ship, so named because it's too large to fit in the Panama Canal. Image source: Wikipedia, licensed under CC BY-SA 4.0.

But, what are the best, most efficient routes for these ships? To a computer scientist, this is a graph theory problem; to a business analyst, a supply chain problem. Done poorly, containers linger at ports, ships idle offshore unable to berth, and ultimately, products become pricier as the flow of physical items becomes slower and unpredictable. Every container shipping company needs to solve these challenges, but they are typically solved separately. Combining them multiplies the complexity, and, to the best of our knowledge, is a problem that has never been solved at the scale required by the largest container operations (500 vessels and 1500 ports). Google's Operations Research team is proud to announce the Shipping Network Design API, which implements a new solution to this problem. Our approach scales better, enabling solutions to world-scale supply chain problems, while being faster than any known previous attempts. It is able to double the profit of a container shipper, deliver 13% more containers, and do so with 15% fewer vessels. Read on to see how we did it.

Background

There are three components to the Liner Shipping Network Design and Scheduling Problem (LSNDSP). Network design determines the order in which vessels visit ports, network scheduling determines the times they arrive and leave, and container routing chooses the journey that containers take from origin to destination. Every container shipping company needs to solve all three challenges, but they are typically solved sequentially. Solving them all simultaneously is more difficult but is also more likely to discover better solutions. Solutions to network design create service lines that a small set of vessels follow: for instance, sailing between eastern Asia, through the Suez canal, and to southern Europe. These service lines are published with dates, so that shippers can know when and where to have their containers ready at port.

Part of a network design for a hypothetical ocean shipper, showing how vessels might cycle between nine ports.

Container vessels can't dock at ports whenever they like; they have pre-arranged berthing slots. As they approach a port, they linger at an anchorage area, an offshore location where they can drop anchor until their berth becomes free. When a port is congested, vessels may end up staying in the anchorage area for hours or days. And so the precise network schedule of the vessels becomes important: not just what day to dock, but what hour, and whether to increase velocity to arrive at a particular time (or conversely, to decrease velocity to save fuel). Once the vessel docks at the port, cranes offload the containers into stacks, and then load other containers onto the vessels for the next leg of the voyage. If the vessel falls behind schedule, it will sometimes cut-and-run: leaving the port before all the intended containers have been loaded, relying on later vessels to pick up the containers. When a container spends time at an intermediate port en route from origin to destination, it’s called transshipment. This multiplies further the number of possible solutions to LSNDSP. Transshipment is just one of several constraints that come into play when generating container routes.

A container vessel berthed at port, underneath the cranes that load and unload containers. Image is in the public domain.

Solving these three problems — network design, network scheduling, and container routing — at scale involves a staggeringly large search space.

Methods

Every optimization problem has three components: variables (e.g., ships and ports), constraints on those variables (e.g., a ship can fit only so many containers onboard), and an objective function to be minimized or maximized (e.g., maximize the number of containers shipped). The variables and constraints are often represented as a matrix in which the columns are the variables and the rows are the constraints. A common technique to decompose such large problems is column generation, in which only a subset of the variables are considered at first, and then new variables — that is, new columns — are generated to more closely approximate the original problem. To help manage this, we developed a software library that analyzes the problem and predicts which columns are best to generate. This library will be open sourced via MathOpt, our mathematical programming framework. With this in hand, we defined two basic approaches to solve the problem: Double Column Generation

We considered network design and container routing as two coupled problems, each consisting of a primary selection problem (choose the best option) and a subsidiary generation problem (identify reasonable options). We applied a shortest-path algorithm to each pair of problems to generate reasonable options, followed by a linear program (using our linear programming solver, Glop) to choose the best options for each. We applied the column generation technique to both at the same time, using intermediate results on each problem to influence progress on the other. This double column generation approach enabled us to find provably optimal solutions, but it only scaled well to moderate sized problems.

CP-SAT

We then tried an implementation based on constraint programming, using our CP-SAT constraint programming solver. This also worked well up to mid-sized networks, but did not scale to the worldwide shipping problem. These two approaches enabled us to find provably optimal solutions, but they only scaled well to small and medium sized problems. To improve their scalability, we applied a heuristic strategy using two variants of local search in which we examine neighborhoods around existing solutions to find opportunities for improvements. Large neighborhood search

We fixed parts of the solution (e.g., ""this vessel will visit Los Angeles on alternate Tuesdays"") before applying either of the methods described above. This improves scalability by reducing the search space.

Variable neighborhood search

We explored neighborhoods over both the network and the schedule. We parallelize the exploration and distribute it over multiple machines to evaluate many neighborhoods simultaneously. This also improves scalability by limiting the search space, while also allowing us to incorporate knowledge from Operations Research and the shipping industry. With both of these approaches, we made use of incrementalism: locking down promising portions of a solution so that we could start from a known good solution to make it better. Finally, we also took into consideration transit times. Previous attempts to solve this problem didn't take transit times into account, since they make the problem much more difficult to solve. We found that inclusion of transit times significantly improved the solution quality.

play silent looping video pause silent looping video A small modification to the network discovered with neighborhood search can affect shipping profits. In this example, the model might suggest a new connection that makes it possible to ship containers between ports that weren’t previously connected.

Results

To quantify the performance of our solutions, we use LINERLIB, an industry benchmark for shipping network design problems, including container shipping scenarios: fleets, ports, and container demands. We tested our solution on three scenarios: WorldSmall, EuropeAsia, and the beastly WorldLarge, the latter of which contains 500 vessels, 200 ports, and nearly 140,000 containers. Below is the comparison of five LINERLIB scenarios.

A comparison of our solutions to the previously best known solutions. The size of the bubble corresponds to the number of ports, indicating the scale of the problem. Our solutions consistently reduce the number of vessels needed for a given volume of containers.

In any optimization scenario, it's important to specify the objective. Want to maximize the number of containers shipped? Easy: throw more ships at the problem, ballooning operating costs. Want to minimize the number of vessels used? Again, easy: have one ship deliver every container in the world, with ludicrously long delivery times. The LINERLIB benchmark balances these by calculating the estimated profit: the revenue from on-time deliveries minus the costs of sailing and handling containers at port. The next figure depicts how our solution's profit compares to previous attempts.

Improvements in weekly profit from our solutions compared to the previously best available solutions. Profits are based on the economic assumptions in the LINERLIB dataset.

Compared to the baseline, our method was able to route more containers with fewer vessels. For each of the LINERLIB scenarios, our solutions improved the overall efficiency, increasing the throughput (35% more containers for WorldSmall, 14% for EuropeAsia, 35% for Pacific, 32% for Mediterranean) while using fewer vessels (7%, 15%, 4%, and 23% respectively). Based on the economic assumptions in LINERLIB, our solutions also improved projected profit margins considerably.

Conclusion"
https://research.google/blog/announcing-scann-efficient-vector-similarity-search/,Announcing ScaNN: Efficient Vector Similarity Search,"Suppose one wants to search through a large dataset of literary works using queries that require an exact match of title, author, or other easily machine-indexable criteria. Such a task would be well suited for a relational database using a language such as SQL. However, if one wants to support more abstract queries, such as “Civil War poem,” it is no longer possible to rely on naive similarity metrics such as the number of words in common between two phrases. For example, the query “science fiction” is more related to “future” than it is to “earth science” despite the former having zero, and the latter having one, word in common with the query.

Machine learning (ML) has greatly improved computers’ abilities to understand language semantics and therefore answer these abstract queries. Modern ML models can transform inputs such as text and images into embeddings, high dimensional vectors trained such that more similar inputs cluster closer together. For a given query, we can therefore compute its embedding, and find the literary works whose embeddings are closest to the query’s. In this manner, ML has transformed an abstract and previously difficult-to-specify task into a rigorous mathematical one. However, a computational challenge remains: for a given query embedding, how does one quickly find the nearest dataset embeddings? The set of embeddings is often too large for exhaustive search and its high dimensionality makes pruning difficult.

In our ICML 2020 paper, “Accelerating Large-Scale Inference with Anisotropic Vector Quantization,” we address this problem by focusing on how to compress the dataset vectors to enable fast approximate distance computations, and propose a new compression technique that significantly boosts accuracy compared to prior works. This technique is utilized in our recently open-sourced vector similarity search library (ScaNN), and enables us to outperform other vector similarity search libraries by a factor of two, as measured on ann-benchmarks.com.





The Importance of Vector Similarity Search

Embedding-based search is a technique that is effective at answering queries that rely on semantic understanding rather than simple indexable properties. In this technique, machine learning models are trained to map the queries and database items to a common vector embedding space, such that the distance between embeddings carries semantic meaning, i.e., similar items are closer together.

The two-tower neural network model, illustrated above, is a specific type of embedding-based search where queries and database items are mapped to the embedding space by two respective neural networks. In this example the model responds to natural-language queries for a hypothetical literary database.

To answer a query with this approach, the system must first map the query to the embedding space. It then must find, among all database embeddings, the ones closest to the query; this is the nearest neighbor search problem. One of the most common ways to define the query-database embedding similarity is by their inner product; this type of nearest neighbor search is known as maximum inner-product search (MIPS).

Because the database size can easily be in the millions or even billions, MIPS is often the computational bottleneck to inference speed, and exhaustive search is impractical. This necessitates the use of approximate MIPS algorithms that exchange some accuracy for a significant speedup over brute-force search.





A New Quantization Approach for MIPS

Several state-of-the-art solutions for MIPS are based on compressing the database items so that an approximation of their inner product can be computed in a fraction of the time taken by brute-force. This compression is commonly done with learned quantization, where a codebook of vectors is trained from the database and is used to approximately represent the database elements.

Previous vector quantization schemes quantized database elements with the aim of minimizing the average distance between each vector x and its quantized form x̃. While this is a useful metric, optimizing for this is not equivalent to optimizing nearest-neighbor search accuracy. The key idea behind our paper is that encodings with higher average distance may actually result in superior MIPS accuracy.

The intuition for our result is illustrated below. Suppose we have two database embeddings x 1 and x 2 , and must quantize each to one of two centers: c 1 or c 2 . Our goal is to quantize each x i to x̃ i such that the inner product <q, x̃ i > is as similar to the original inner product <q, x i > as possible. This can be visualized as making the magnitude of the projection of x̃ i onto q as similar as possible to the projection of x i onto q. In the traditional approach to quantization (left), we would pick the closest center for each x i , which leads to an incorrect relative ranking of the two points: <q, x̃ 1 > is greater than <q, x̃ 2 >, even though <q, x 1 > is less than <q, x 2 >! If we instead assign x 1 to c 1 and x 2 to c 2 , we get the correct ranking. This is illustrated in the figure below.

The goal is to quantize each x i to x̃ i = c 1 or x̃ i = c 2 . Traditional quantization (left) results in the incorrect ordering of x 1 and x 2 for this query. Even though our approach (right) chooses centers farther away from the data points, this in fact leads to lower inner product error and higher accuracy.

It turns out that direction matters as well as magnitude--even though c 1 is farther from x 1 than c 2 , c 1 is offset from x 1 in a direction almost entirely orthogonal to x 1 , while c 2 ’s offset is parallel (for x 2 , the same situation applies but flipped). Error in the parallel direction is much more harmful in the MIPS problem because it disproportionately impacts high inner products, which by definition are the ones that MIPS is trying to estimate accurately.

Based on this intuition, we more heavily penalize quantization error that is parallel to the original vector. We refer to our novel quantization technique as anisotropic vector quantization due to the directional dependence of its loss function. The ability of this technique to trade increased quantization error of lower inner products in exchange for superior accuracy for high inner products is the key innovation and the source of its performance gains.

In the above diagrams, ellipses denote contours of equal loss. In anisotropic vector quantization, error parallel to the original data point x is penalized more.

Anisotropic Vector Quantization in ScaNN

Anisotropic vector quantization allows ScaNN to better estimate inner products that are likely to be in the top-k MIPS results and therefore achieve higher accuracy. On the glove-100-angular benchmark from ann-benchmarks.com, ScaNN outperformed eleven other carefully tuned vector similarity search libraries, handling roughly twice as many queries per second for a given accuracy as the next-fastest library.*

Recall@k is a commonly used metric for nearest neighbor search accuracy, which measures the proportion of the true nearest k neighbors that are present in an algorithm’s returned k neighbors. ScaNN (upper purple line) consistently achieves superior performance across various points of the speed-accuracy trade-off.

ScaNN is open-source software and you can try it yourself at GitHub. The library can be directly installed via Pip and has interfaces for both TensorFlow and Numpy inputs. Please see the GitHub repository for further instructions on installing and configuring ScaNN.





Conclusion

By modifying the vector quantization objective to align with the goals of MIPS, we achieve state-of-the-art performance on nearest neighbor search benchmarks, a key indicator of embedding-based search performance. Although anisotropic vector quantization is an important technique, we believe it is just one example of the performance gains achievable by optimizing algorithms for the end goal of improving search accuracy rather than an intermediate goal such as compression distortion.





Acknowledgements

This post reflects the work of the entire ScaNN team: David Simcha, Erik Lindgren, Felix Chern, Nathan Cordeiro, Ruiqi Guo, Sanjiv Kumar, and Zonglin Li. We’d also like to thank Dan Holtmann-Rice, Dave Dopson, and Felix Yu.



* ScaNN performs similarly well on the other datasets of ann-benchmarks.com, but the website currently shows outdated, lower numbers. See this pull request for more representative performance figures on other datasets. ↩

"
https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/,How to add a directory to your PATH,"I was talking to a friend about how to add a directory to your PATH today. It’s something that feels “obvious” to me since I’ve been using the terminal for a long time, but when I searched for instructions for how to do it, I actually couldn’t find something that explained all of the steps – a lot of them just said “add this to ~/.bashrc”, but what if you’re not using bash? What if your bash config is actually in a different file? And how are you supposed to figure out which directory to add anyway?

So I wanted to try to write down some more complete directions and mention some of the gotchas I’ve run into over the years.

Here’s a table of contents:

step 1: what shell are you using?

step 2: find your shell’s config file

a note on bash’s config file

step 3: figure out which directory to add

step 3.1: double check it’s the right directory

step 4: edit your shell config

step 5: restart your shell

problems:

problem 1: it ran the wrong program

problem 2: the program isn’t being run from your shell

problem 3: duplicate PATH entries making it harder to debug

problem 4: losing your history after updating your PATH

notes:

a note on source

a note on fish_add_path

step 1: what shell are you using?

If you’re not sure what shell you’re using, here’s a way to find out. Run this:

ps -p $$ -o pid,comm=

if you’re using bash, it’ll print out 97295 bash

if you’re using zsh, it’ll print out 97295 zsh

if you’re using fish, it’ll print out an error like “In fish, please use $fish_pid” ($$ isn’t valid syntax in fish, but in any case the error message tells you that you’re using fish, which you probably already knew)

Also bash is the default on Linux and zsh is the default on Mac OS (as of 2024). I’ll only cover bash, zsh, and fish in these directions.

step 2: find your shell’s config file

in zsh, it’s probably ~/.zshrc

in bash, it might be ~/.bashrc, but it’s complicated, see the note in the next section

in fish, it’s probably ~/.config/fish/config.fish (you can run echo $__fish_config_dir if you want to be 100% sure)

a note on bash’s config file

Bash has three possible config files: ~/.bashrc, ~/.bash_profile, and ~/.profile.

If you’re not sure which one your system is set up to use, I’d recommend testing this way:

add echo hi there to your ~/.bashrc

Restart your terminal

If you see “hi there”, that means ~/.bashrc is being used! Hooray!

Otherwise remove it and try the same thing with ~/.bash_profile

You can also try ~/.profile if the first two options don’t work.

(there are a lot of elaborate flow charts out there that explain how bash decides which config file to use but IMO it’s not worth it to internalize them and just testing is the fastest way to be sure)

step 3: figure out which directory to add

Let’s say that you’re trying to install and run a program called http-server and it doesn’t work, like this:

$ npm install -g http-server $ http-server bash: http-server: command not found

How do you find what directory http-server is in? Honestly in general this is not that easy – often the answer is something like “it depends on how npm is configured”. A few ideas:

Often when setting up a new installer (like cargo, npm, homebrew, etc), when you first set it up it’ll print out some directions about how to update your PATH. So if you’re paying attention you can get the directions then.

Sometimes installers will automatically update your shell’s config file to update your PATH for you

Sometimes just Googling “where does npm install things?” will turn up the answer

Some tools have a subcommand that tells you where they’re configured to install things, like:

Node/npm: npm config get prefix (then append /bin/)

Go: go env GOPATH (then append /bin/)

asdf: asdf info | grep ASDF_DIR (then append /bin/ and /shims/)

step 3.1: double check it’s the right directory

Once you’ve found a directory you think might be the right one, make sure it’s actually correct! For example, I found out that on my machine, http-server is in ~/.npm-global/bin. I can make sure that it’s the right directory by trying to run the program http-server in that directory like this:

$ ~/.npm-global/bin/http-server Starting up http-server, serving ./public

It worked! Now that you know what directory you need to add to your PATH, let’s move to the next step!

step 4: edit your shell config

Now we have the 2 critical pieces of information we need:

Which directory you’re trying to add to your PATH (like ~/.npm-global/bin/)

Where your shell’s config is (like ~/.bashrc, ~/.zshrc, or ~/.config/fish/config.fish)

Now what you need to add depends on your shell:

bash instructions:

Open your shell’s config file, and add a line like this:

export PATH=$PATH:~/.npm-global/bin/

(obviously replace ~/.npm-global/bin with the actual directory you’re trying to add)

zsh instructions:

You can do the same thing as in bash, but zsh also has some slightly fancier syntax you can use if you prefer:

path=( $path ~/.npm-global/bin )

fish instructions:

In fish, the syntax is different:

set PATH $PATH ~/.npm-global/bin

(in fish you can also use fish_add_path, some notes on that further down)

step 5: restart your shell

Now, an extremely important step: updating your shell’s config won’t take effect if you don’t restart it!

Two ways to do this:

open a new terminal (or terminal tab), and maybe close the old one so you don’t get confused

Run bash to start a new shell (or zsh if you’re using zsh, or fish if you’re using fish)

I’ve found that both of these usually work fine.

And you should be done! Try running the program you were trying to run and hopefully it works now.

If not, here are a couple of problems that you might run into:

problem 1: it ran the wrong program

If the wrong version of a program is running, you might need to add the directory to the beginning of your PATH instead of the end.

For example, on my system I have two versions of python3 installed, which I can see by running which -a:

$ which -a python3 /usr/bin/python3 /opt/homebrew/bin/python3

The one your shell will use is the first one listed.

If you want to use the Homebrew version, you need to add that directory (/opt/homebrew/bin) to the beginning of your PATH instead, by putting this in your shell’s config file (it’s /opt/homebrew/bin/:$PATH instead of the usual $PATH:/opt/homebrew/bin/)

export PATH=/opt/homebrew/bin/:$PATH

or in fish:

set PATH ~/.cargo/bin $PATH

problem 2: the program isn’t being run from your shell

All of these directions only work if you’re running the program from your shell. If you’re running the program from an IDE, from a GUI, in a cron job, or some other way, you’ll need to add the directory to your PATH in a different way, and the exact details might depend on the situation.

in a cron job

Some options:

use the full path to the program you’re running, like /home/bork/bin/my-program

put the full PATH you want as the first line of your crontab (something like PATH=/bin:/usr/bin:/usr/local/bin:….). You can get the full PATH you’re using in your shell by running echo ""PATH=$PATH"".

I’m honestly not sure how to handle it in an IDE/GUI because I haven’t run into that in a long time, will add directions here if someone points me in the right direction.

problem 3: duplicate PATH entries making it harder to debug

If you edit your path and start a new shell by running bash (or zsh, or fish), you’ll often end up with duplicate PATH entries, because the shell keeps adding new things to your PATH every time you start your shell.

Personally I don’t think I’ve run into a situation where this kind of duplication breaks anything, but the duplicates can make it harder to debug what’s going on with your PATH if you’re trying to understand its contents.

Some ways you could deal with this:

If you’re debugging your PATH, open a new terminal to do it in so you get a “fresh” state. This should avoid the duplication.

Deduplicate your PATH at the end of your shell’s config (for example in zsh apparently you can do this with typeset -U path)

Check that the directory isn’t already in your PATH when adding it (for example in fish I believe you can do this with fish_add_path --path /some/directory)

How to deduplicate your PATH is shell-specific and there isn’t always a built in way to do it so you’ll need to look up how to accomplish it in your shell.

problem 4: losing your history after updating your PATH

Here’s a situation that’s easy to get into in bash or zsh:

Run a command (it fails)

Update your PATH

Run bash to reload your config

Press the up arrow a couple of times to rerun the failed command (or open a new terminal)

The failed command isn’t in your history! Why not?

This happens because in bash, by default, history is not saved until you exit the shell.

Some options for fixing this:

Instead of running bash to reload your config, run source ~/.bashrc (or source ~/.zshrc in zsh). This will reload the config inside your current session.

Configure your shell to continuously save your history instead of only saving the history when the shell exits. (How to do this depends on whether you’re using bash or zsh, the history options in zsh are a bit complicated and I’m not exactly sure what the best way is)

a note on source

When you install cargo (Rust’s installer) for the first time, it gives you these instructions for how to set up your PATH, which don’t mention a specific directory at all.

This is usually done by running one of the following (note the leading DOT): . ""$HOME/.cargo/env"" # For sh/bash/zsh/ash/dash/pdksh source ""$HOME/.cargo/env.fish"" # For fish

The idea is that you add that line to your shell’s config, and their script automatically sets up your PATH (and potentially other things) for you.

This is pretty common (for example Homebrew suggests you eval brew shellenv), and there are two ways to approach this:

Just do what the tool suggests (like adding . ""$HOME/.cargo/env"" to your shell’s config)

Figure out which directories the script they’re telling you to run would add to your PATH, and then add those manually. Here’s how I’d do that:

Run . ""$HOME/.cargo/env"" in my shell (or the fish version if using fish)

Run echo ""$PATH"" | tr ':' '\n' | grep cargo to figure out which directories it added

See that it says /Users/bork/.cargo/bin and shorten that to ~/.cargo/bin

Add the directory ~/.cargo/bin to PATH (with the directions in this post)

I don’t think there’s anything wrong with doing what the tool suggests (it might be the “best way”!), but personally I usually use the second approach because I prefer knowing exactly what configuration I’m changing.

a note on fish_add_path

fish has a handy function called fish_add_path that you can run to add a directory to your PATH like this:

fish_add_path /some/directory

This is cool (it’s such a simple command!) but I’ve stopped using it for a couple of reasons:

Sometimes fish_add_path will update the PATH for every session in the future (with a “universal variable”) and sometimes it will update the PATH just for the current session and it’s hard for me to tell which one it will do. In theory the docs explain this but I could not understand them.

If you ever need to remove the directory from your PATH a few weeks or months later because maybe you made a mistake, it’s kind of hard to do (there are instructions in this comments of this github issue though).

that’s all

Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if you there are other major gotchas that have tripped you up when adding a directory to your PATH, or if you have questions about this post!"
https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/,Some terminal frustrations,"A few weeks ago I ran a terminal survey (you can read the results here) and at the end I asked:

What’s the most frustrating thing about using the terminal for you?

1600 people answered, and I decided to spend a few days categorizing all the responses. Along the way I learned that classifying qualitative data is not easy but I gave it my best shot. I ended up building a custom tool to make it faster to categorize everything.

As with all of my surveys the methodology isn’t particularly scientific. I just posted the survey to Mastodon and Twitter, ran it for a couple of days, and got answers from whoever happened to see it and felt like responding.

Here are the top categories of frustrations!

I think it’s worth keeping in mind while reading these comments that

40% of people answering this survey have been using the terminal for 21+ years

95% of people answering the survey have been using the terminal for at least 4 years

These comments aren’t coming from total beginners.

Here are the categories of frustrations! The number in brackets is the number of people with that frustration. I’m mostly writing this up for myself because I’m trying to write a zine about the terminal and I wanted to get a sense for what people are having trouble with.

remembering syntax (115)

People talked about struggles remembering:

the syntax for CLI tools like awk, jq, sed, etc

the syntax for redirects

keyboard shortcuts for tmux, text editing, etc

One example comment:

There are just so many little “trivia” details to remember for full functionality. Even after all these years I’ll sometimes forget where it’s 2 or 1 for stderr, or forget which is which for > and >>.

switching terminals is hard (91)

People talked about struggling with switching systems (for example home/work computer or when SSHing) and running into:

OS differences in keyboard shortcuts (like Linux vs Mac)

systems which don’t have their preferred text editor (“no vim” or “only vim”)

different versions of the same command (like Mac OS grep vs GNU grep)

no tab completion

a shell they aren’t used to (“the subtle differences between zsh and bash”)

as well as differences inside the same system like pagers being not consistent with each other (git diff pagers, other pagers).

One example comment:

I got used to fish and vi mode which are not available when I ssh into servers, containers.

color (85)

Lots of problems with color, like:

programs setting colors that are unreadable with a light background color

finding a colorscheme they like (and getting it to work consistently across different apps)

color not working inside several layers of SSH/tmux/etc

not liking the defaults

not wanting color at all and struggling to turn it off

This comment felt relatable to me:

Getting my terminal theme configured in a reasonable way between the terminal emulator and fish (I did this years ago and remember it being tedious and fiddly and now feel like I’m locked into my current theme because it works and I dread touching any of that configuration ever again).

keyboard shortcuts (84)

Half of the comments on keyboard shortcuts were about how on Linux/Windows, the keyboard shortcut to copy/paste in the terminal is different from in the rest of the OS.

Some other issues with keyboard shortcuts other than copy/paste:

using Ctrl-W in a browser-based terminal and closing the window

the terminal only supports a limited set of keyboard shortcuts (no Ctrl-Shift-, no Super, no Hyper, lots of ctrl- shortcuts aren’t possible like Ctrl-,)

the OS stopping you from using a terminal keyboard shortcut (like by default Mac OS uses Ctrl+left arrow for something else)

issues using emacs in the terminal

backspace not working (2)

other copy and paste issues (75)

Aside from “the keyboard shortcut for copy and paste is different”, there were a lot of OTHER issues with copy and paste, like:

copying over SSH

how tmux and the terminal emulator both do copy/paste in different ways

dealing with many different clipboards (system clipboard, vim clipboard, the “middle click” clipboard on Linux, tmux’s clipboard, etc) and potentially synchronizing them

random spaces added when copying from the terminal

pasting multiline commands which automatically get run in a terrifying way

wanting a way to copy text without using the mouse

discoverability (55)

There were lots of comments about this, which all came down to the same basic complaint – it’s hard to discover useful tools or features! This comment kind of summed it all up:

How difficult it is to learn independently. Most of what I know is an assorted collection of stuff I’ve been told by random people over the years.

steep learning curve (44)

A lot of comments about it generally having a steep learning curve. A couple of example comments:

After 15 years of using it, I’m not much faster than using it than I was 5 or maybe even 10 years ago.

and

That I know I could make my life easier by learning more about the shortcuts and commands and configuring the terminal but I don’t spend the time because it feels overwhelming.

history (42)

Some issues with shell history:

history not being shared between terminal tabs (16)

limits that are too short (4)

history not being restored when terminal tabs are restored

losing history because the terminal crashed

not knowing how to search history

One example comment:

It wasted a lot of time until I figured it out and still annoys me that “history” on zsh has such a small buffer; I have to type “history 0” to get any useful length of history.

bad documentation (37)

People talked about:

documentation being generally opaque

lack of examples in man pages

programs which don’t have man pages

Here’s a representative comment:

Finding good examples and docs. Man pages often not enough, have to wade through stack overflow

scrollback (36)

A few issues with scrollback:

programs printing out too much data making you lose scrollback history

resizing the terminal messes up the scrollback

lack of timestamps

GUI programs that you start in the background printing stuff out that gets in the way of other programs’ outputs

One example comment:

When resizing the terminal (in particular: making it narrower) leads to broken rewrapping of the scrollback content because the commands formatted their output based on the terminal window width.

Lots of comments about how the terminal feels hampered by legacy decisions and how users often end up needing to learn implementation details that feel very esoteric. One example comment:

Most of the legacy cruft, it would be great to have a green field implementation of the CLI interface.

shell scripting (32)

Lots of complaints about POSIX shell scripting. There’s a general feeling that shell scripting is difficult but also that switching to a different less standard scripting language (fish, nushell, etc) brings its own problems.

Shell scripting. My tolerance to ditch a shell script and go to a scripting language is pretty low. It’s just too messy and powerful. Screwing up can be costly so I don’t even bother.

more issues

Some more issues that were mentioned at least 10 times:

(31) inconsistent command line arguments: is it -h or help or –help?

(24) keeping dotfiles in sync across different systems

(23) performance (e.g. “my shell takes too long to start”)

(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)

(17) generally feeling scared/uneasy (“The debilitating fear that I’m going to do some mysterious Bad Thing with a command and I will have absolutely no idea how to fix or undo it or even really figure out what happened”)

(16) terminfo issues (“Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.”)

(16) lack of image support (sixel etc)

(15) SSH issues (like having to start over when you lose the SSH connection)

(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)

(15) typos & slow typing

(13) the terminal getting messed up for various reasons (pressing Ctrl-S, cating a binary, etc)

(12) quoting/escaping in the shell

(11) various Windows/PowerShell issues

n/a (122)

There were also 122 answers to the effect of “nothing really” or “only that I can’t do EVERYTHING in the terminal”

One example comment:

Think I’ve found work arounds for most/all frustrations

that’s all!

I’m not going to make a lot of commentary on these results, but here are a couple of categories that feel related to me:

remembering syntax & history (often the thing you need to remember is something you’ve run before!)

discoverability & the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)

“switching systems is hard” & “it feels outdated” (tools that haven’t really changed in 30 or 40 years have many problems but they do tend to be always there no matter what system you’re on, which is very useful and makes them hard to stop using)

Trying to categorize all these results in a reasonable way really gave me an appreciation for social science researchers’ skills."
https://martinfowler.com/articles/continuousIntegration.html,Continuous Integration,"Building a Feature with Continuous Integration

The easiest way for me to explain what Continuous Integration is and how it works is to show a quick example of how it works with the development of a small feature. I'm currently working with a major manufacturer of magic potions, we are extending their product quality system to calculate how long the potion's effect will last. We already have a dozen potions supported in the system, and we need to extend the logic for flying potions. (We've learned that having them wear off too early severely impacts customer retention.) Flying potions introduce a few new factors to take care of, one of which is the moon phase during secondary mixing.

local environment

central repo and environment

teammates

I pull from the central store to ensure my local copy is up-to-date

I make my changes to the code base, until I'm ready and tests are green…

…meanwhile colleagues update the central repo with their commits

I pull collaborators' changes from the central store and combine them with my changes

I rebuild and check tests are green

I push my changes to the central repo

A service builds the product on a central environment

The central system notifies me that all was well with the central build

my colleagues will combine their changes when they prepare to push their next changes

I begin by taking a copy of the latest product sources onto my local development environment. I do this by checking out the current mainline from the central repository with git pull.

Once the source is in my environment, I execute a command to build the product. This command checks that my environment is set up correctly, does any compilation of the sources into an executable product, starts the product, and runs a comprehensive suite of tests against it. This should take only a few minutes, while I start poking around the code to decide how to begin adding the new feature. This build hardly ever fails, but I do it just in case, because if it does fail, I want to know before I start making changes. If I make changes on top of a failing build, I'll get confused thinking it was my changes that caused the failure.

Now I take my working copy and do whatever I need to do to deal with the moon phases. This will consist of both altering the product code, and also adding or changing some of the automated tests. During that time I run the automated build and tests frequently. After an hour or so I have the moon logic incorporated and tests updated.

I'm now ready to integrate my changes back into the central repository. My first step for this is to pull again, because it's possible, indeed likely, that my colleagues will have pushed changes into the mainline while I've been working. Indeed there are a couple of such changes, which I pull into my working copy. I combine my changes on top of them and run the build again. Usually this feels superfluous, but this time a test fails. The test gives me some clue about what's gone wrong, but I find it more useful to look at the commits that I pulled to see what changed. It seems that someone has made an adjustment to a function, moving some of its logic out into its callers. They fixed all the callers in the mainline code, but I added a new call in my changes that, of course, they couldn't see yet. I make the same adjustment and rerun the build, which passes this time.

Since I was a few minutes sorting that out, I pull again, and again there's a new commit. However the build works fine with this one, so I'm able to git push my change up to the central repository.

However my push doesn't mean I'm done. Once I've pushed to the mainline a Continuous Integration Service notices my commit, checks out the changed code onto a CI agent, and builds it there. Since the build was fine in my environment I don't expect it to fail on the CI Service, but there is a reason that “works on my machine” is a well-known phrase in programmer circles. It's rare that something gets missed that causes the CI Services build to fail, but rare is not the same as never.

The integration machine's build doesn't take long, but it's long enough that an eager developer would be starting to think about the next step in calculating flight time. But I'm an old guy, so enjoy a few minutes to stretch my legs and read an email. I soon get a notification from the CI service that all is well, so I start the process again for the next part of the change.

Practices of Continuous Integration

The story above is an illustration of Continuous Integration that hopefully gives you a feel of what it's like for an ordinary programmer to work with. But, as with anything, there's quite a few things to sort out when doing this in daily work. So now we'll go through the key practices that we need to do.

Put everything in a version controlled mainline

These days almost every software team keeps their source code in a version control system, so that every developer can easily find not just the current state of the product, but all the changes that have been made to the product. Version control tools allow a system to be rolled back to any point in its development, which can be very helpful to understand the history of the system, using Diff Debugging to find bugs. As I write this, the dominant version control system is git.

But while version control is commonplace, some teams fail to take full advantage of version control. My test for full version control is that I should be able to walk up with a very minimally configured environment - say a laptop with no more than the vanilla operating system installed - and be able to easily build, and run the product after cloning the repository. This means the repository should reliably return product source code, tests, database schema, test data, configuration files, IDE configurations, install scripts, third-party libraries, and any tools required to build the software.

I should be able to walk up with a laptop loaded with only an operating system, and by using the repository, obtain everything I need to build and run the product.

You might notice I said that the repository should return all of these elements, which isn't the same as storing them. We don't have to store the compiler in the repository, but we need to be able to get at the right compiler. If I check out last year's product sources, I may need to be able to build them with the compiler I was using last year, not the version I'm using now. The repository can do this by storing a link to immutable asset storage - immutable in the sense that once an asset is stored with an id, I'll always get exactly that asset back again. I can also do this with library code, providing I both trust the asset storage and always reference a particular version, never “the latest version”.

Similar asset storage schemes can be used for anything too large, such as videos. Cloning a repository often means grabbing everything, even if it's not needed. By using references to an asset store, the build scripts can choose to download only what's needed for a particular build.

In general we should store in source control everything we need to build anything, but nothing that we actually build. Some people do keep the build products in source control, but I consider that to be a smell - an indication of a deeper problem, usually an inability to reliably recreate builds. It can be useful to cache build products, but they should always be treated as disposable, and it's usually good to then ensure they are removed promptly so that people don't rely on them when they shouldn't.

A second element of this principle is that it should be easy to find the code for a given piece of work. Part of this is clear names and URL schemes, both within the repository and within the broader enterprise. It also means not having to spend time figuring out which branch within the version control system to use. Continuous Integration relies on having a clear mainline - a single, shared, branch that acts as the current state of the product. This is the next version that will be deployed to production.

Teams that use git mostly use the name “main” for the mainline branch, but we also sometimes see “trunk” or the old default of “master”. The mainline is that branch on the central repository, so to add a commit to a mainline called main I need to first commit to my local copy of main and then push that commit to the central server. The tracking branch (called something like origin/main) is a copy of the mainline on my local machine. However it may be out of date, since in a Continuous Integration environment there are many commits pushed into mainline every day.

As much as possible, we should use text files to define the product and its environment. I say this because, although version-control systems can store and track non-text files, they don't usually provide any facility to easily see the difference between versions. This makes it much harder to understand what change was made. It's possible that in the future we'll see more storage formats having the facility to create meaningful diffs, but at the moment clear diffs are almost entirely reserved for text formats. Even there we need to use text formats that will produce comprehensible diffs.

Automate the Build

Turning the source code into a running system can often be a complicated process involving compilation, moving files around, loading schemas into databases, and so on. However like most tasks in this part of software development it can be automated - and as a result should be automated. Asking people to type in strange commands or clicking through dialog boxes is a waste of time and a breeding ground for mistakes.

Computers are designed to perform simple, repetitive tasks. As soon as you have humans doing repetitive tasks on behalf of computers, all the computers get together late at night and laugh at you.

-- Neal Ford

Most modern programming environments include tooling for automating builds, and such tools have been around for a long time. I first encountered them with make, one of the earliest Unix tools.

Any instructions for the build need to be stored in the repository, in practice this means that we must use text representations. That way we can easily inspect them to see how they work, and crucially, see diffs when they change. Thus teams using Continuous Integration avoid tools that require clicking around in UIs to perform a build or to configure an environment.

It's possible to use a regular programming language to automate builds, indeed simple builds are often captured as shell scripts. But as builds get more complicated it's better to use a tool that's designed with build automation in mind. Partly this is because such tools will have built-in functions for common build tasks. But the main reason is that build tools work best with a particular way to organize their logic - an alternative computational model that I refer to as a Dependency Network. A dependency network organizes its logic into tasks which are structured as a graph of dependencies.

A trivially simple dependency network might say that the “test” task is dependent upon the “compile” task. If I invoke the test task, it will look to see if the compile task needs to be run and if so invoke it first. Should the compile task itself have dependencies, the network will look to see if it needs to invoke them first, and so on backwards along the dependency chain. A dependency network like this is useful for build scripts because often tasks take a long time, which is wasted if they aren't needed. If nobody has changed any source files since I last ran the tests, then I can save doing a potentially long compilation.

To tell if a task needs to be run, the most common and straightforward way is to look at the modification times of files. If any of the input files to the compilation have been modified later than the output, then we know the compilation needs to be executed if that task is invoked.

A common mistake is not to include everything in the automated build. The build should include getting the database schema out of the repository and firing it up in the execution environment. I'll elaborate my earlier rule of thumb: anyone should be able to bring in a clean machine, check the sources out of the repository, issue a single command, and have a running system on their own environment.

While a simple program may only need a line or two of script file to build, complex systems often have a large graph of dependencies, finely tuned to minimize the amount of time required to build things. This website, for example, has over a thousand web pages. My build system knows that should I alter the source for this page, I only have to build this one page. But should I alter a core file in the publication tool chain, then it needs to rebuild them all. Either way, I invoke the same command in my editor, and the build system figures out how much to do.

Depending on what we need, we may need different kinds of things to be built. We can build a system with or without test code, or with different sets of tests. Some components can be built stand-alone. A build script should allow us to build alternative targets for different cases.

Make the Build Self-Testing

Traditionally a build meant compiling, linking, and all the additional stuff required to get a program to execute. A program may run, but that doesn't mean it does the right thing. Modern statically typed languages can catch many bugs, but far more slip through that net. This is a critical issue if we want to integrate as frequently as Continuous Integration demands. If bugs make their way into the product, then we are faced with the daunting task of performing bug fixes on a rapidly-changing code base. Manual testing is too slow to cope with the frequency of change.

Faced with this, we need to ensure that bugs don't get into the product in the first place. The main technique to do this is a comprehensive test suite, one that is run before each integration to flush out as many bugs as possible. Testing isn't perfect, of course, but it can catch a lot of bugs - enough to be useful. Early computers I used did a visible memory self-test when they were booting up, which led me referring to this as Self Testing Code.

Writing self-testing code affects a programmer's workflow. Any programming task combines both modifying the functionality of the program, and also augmenting the test suite to verify this changed behavior. A programmer's job isn't done merely when the new feature is working, but also when they have automated tests to prove it.

Over the two decades since the first version of this article, I've seen programming environments increasingly embrace the need to provide the tools for programmers to build such test suites. The biggest push for this was JUnit, originally written by Kent Beck and Erich Gamma, which had a marked impact on the Java community in the late 1990s. This inspired similar testing frameworks for other languages, often referred to as Xunit frameworks. These stressed a light-weight, programmer-friendly mechanics that allowed a programmer to easily build tests in concert with the product code. Often these tools have some kind of graphical progress bar that is green if the tests pass, but turns red should any fail - leading to phrases like “green build”, or “red-bar”.

A sound test suite would never allow a mischievous imp to do any damage without a test turning red.

The test of such a test suite is that we should be confident that if the tests are green, then no significant bugs are in the product. I like to imagine a mischievous imp that is able to make simple modifications to the product code, such as commenting out lines, or reversing conditionals, but is not able to change the tests. A sound test suite would never allow the imp to do any damage without a test turning red. And any test failing is enough to fail the build, 99.9% green is still red.

Self-testing code is so important to Continuous Integration that it is a necessary prerequisite. Often the biggest barrier to implementing Continuous Integration is insufficient skill at testing.

That self-testing code and Continuous Integration are so tied together is no surprise. Continuous Integration was originally developed as part of Extreme Programming and testing has always been a core practice of Extreme Programming. This testing is often done in the form of Test Driven Development (TDD), a practice that instructs us to never write new code unless it fixes a test that we've written just before. TDD isn't essential for Continuous Integration, as tests can be written after production code as long as they are done before integration. But I do find that, most of the time, TDD is the best way to write self-testing code.

The tests act as an automated check of the health of the code base, and while tests are the key element of such an automated verification of the code, many programming environments provide additional verification tools. Linters can detect poor programming practices, and ensure code follows a team's preferred formatting style, vulnerability scanners can find security weaknesses. Teams should evaluate these tools to include them in the verification process.

Of course we can't count on tests to find everything. As it's often been said: tests don't prove the absence of bugs. However perfection isn't the only point at which we get payback for a self-testing build. Imperfect tests, run frequently, are much better than perfect tests that are never written at all.

Everyone Pushes Commits To the Mainline Every Day

No code sits unintegrated for more than a couple of hours.

-- Kent Beck

Integration is primarily about communication. Integration allows developers to tell other developers about the changes they have made. Frequent communication allows people to know quickly as changes develop.

The one prerequisite for a developer committing to the mainline is that they can correctly build their code. This, of course, includes passing the build tests. As with any commit cycle the developer first updates their working copy to match the mainline, resolves any conflicts with the mainline, then builds on their local machine. If the build passes, then they are free to push to the mainline.

If everyone pushes to the mainline frequently, developers quickly find out if there's a conflict between two developers. The key to fixing problems quickly is finding them quickly. With developers committing every few hours a conflict can be detected within a few hours of it occurring, at that point not much has happened and it's easy to resolve. Conflicts that stay undetected for weeks can be very hard to resolve.

Conflicts in the codebase come in different forms. The easiest to find and resolve are textual conflicts, often called “merge conflicts”, when two developers edit the same fragment of code in different ways. Version-control tools detect these easily once the second developer pulls the updated mainline into their working copy. The harder problem are Semantic Conflicts. If my colleague changes the name of a function and I call that function in my newly added code, the version-control system can't help us. In a statically typed language we get a compilation failure, which is pretty easy to detect, but in a dynamic language we get no such help. And even statically-typed compilation doesn't help us when a colleague makes a change to the body of a function that I call, making a subtle change to what it does. This is why it's so important to have self-testing code.

A test failure alerts that there's a conflict between changes, but we still have to figure out what the conflict is and how to resolve it. Since there's only a few hours of changes between commits, there's only so many places where the problem could be hiding. Furthermore since not much has changed we can use Diff Debugging to help us find the bug.

My general rule of thumb is that every developer should commit to the mainline every day. In practice, those experienced with Continuous Integration integrate more frequently than that. The more frequently we integrate, the less places we have to look for conflict errors, and the more rapidly we fix conflicts.

Frequent commits encourage developers to break down their work into small chunks of a few hours each. This helps track progress and provides a sense of progress. Often people initially feel they can't do something meaningful in just a few hours, but we've found that mentoring and practice helps us learn.

Every Push to Mainline Should Trigger a Build

If everyone on the team integrates at least daily, this ought to mean that the mainline stays in a healthy state. In practice, however, things still do go wrong. This may be due to lapses in discipline, neglecting to update and build before a push, there may also be environmental differences between developer workspaces.

We thus need to ensure that every commit is verified in a reference environment. The usual way to do this is with a Continuous Integration Service (CI Service) that monitors the mainline. (Examples of CI Services are tools like Jenkins, GitHub Actions, Circle CI etc.) Every time the mainline receives a commit, the CI service checks out the head of the mainline into an integration environment and performs a full build. Only once this integration build is green can the developer consider the integration to be complete. By ensuring we have a build with every push, should we get a failure, we know that the fault lies in that latest push, narrowing down where have to look to fix it.

I want to stress here that when we use a CI Service, we only use it on the mainline, which is the main branch on the reference instance of the version control system. It's common to use a CI service to monitor and build from multiple branches, but the whole point of integration is to have all commits coexisting on a single branch. While it may be useful to use CI service to do an automated build for different branches, that's not the same as Continuous Integration, and teams using Continuous Integration will only need the CI service to monitor a single branch of the product.

While almost all teams use CI Services these days, it is perfectly possible to do Continuous Integration without one. Team members can manually check out the head on the mainline onto an integration machine and perform a build to verify the integration. But there's little point in a manual process when automation is so freely available.

(This is an appropriate point to mention that my colleagues at Thoughtworks, have contributed a lot of open-source tooling for Continuous Integration, in particular Cruise Control - the first CI Service.)

Fix Broken Builds Immediately

Continuous Integration can only work if the mainline is kept in a healthy state. Should the integration build fail, then it needs to be fixed right away. As Kent Beck puts it: “nobody has a higher priority task than fixing the build”. This doesn't mean that everyone on the team has to stop what they are doing in order to fix the build, usually it only needs a couple of people to get things working again. It does mean a conscious prioritization of a build fix as an urgent, high priority task

Usually the best way to fix the build is to revert the faulty commit from the mainline, allowing the rest of the team to continue working.

Usually the best way to fix the build is to revert the latest commit from the mainline, taking the system back to the last-known good build. If the cause of the problem is immediately obvious then it can be fixed directly with a new commit, but otherwise reverting the mainline allows some folks to figure out the problem in a separate development environment, allowing the rest of the team to continue to work with the mainline.

Some teams prefer to remove all risk of breaking the mainline by using a Pending Head (also called Pre-tested, Delayed, or Gated Commit.) To do this the CI service needs to set things up so that commits pushed to the mainline for integration do not immediately go onto the mainline. Instead they are placed on another branch until the build completes and only migrated to the mainline after a green build. While this technique avoids any danger to mainline breaking, an effective team should rarely see a red mainline, and on the few times it happens its very visibility encourages folks to learn how to avoid it.

Keep the Build Fast

The whole point of Continuous Integration is to provide rapid feedback. Nothing sucks the blood of Continuous Integration more than a build that takes a long time. Here I must admit a certain crotchety old guy amusement at what's considered to be a long build. Most of my colleagues consider a build that takes an hour to be totally unreasonable. I remember teams dreaming that they could get it so fast - and occasionally we still run into cases where it's very hard to get builds to that speed.

For most projects, however, the XP guideline of a ten minute build is perfectly within reason. Most of our modern projects achieve this. It's worth putting in concentrated effort to make it happen, because every minute chiseled off the build time is a minute saved for each developer every time they commit. Since Continuous Integration demands frequent commits, this adds up to a lot of the time.

If we're staring at a one hour build time, then getting to a faster build may seem like a daunting prospect. It can even be daunting to work on a new project and think about how to keep things fast. For enterprise applications, at least, we've found the usual bottleneck is testing - particularly tests that involve external services such as a database.

Probably the most crucial step is to start working on setting up a Deployment Pipeline. The idea behind a deployment pipeline (also known as build pipeline or staged build) is that there are in fact multiple builds done in sequence. The commit to the mainline triggers the first build - what I call the commit build. The commit build is the build that's needed when someone pushes commits to the mainline. The commit build is the one that has to be done quickly, as a result it will take a number of shortcuts that will reduce the ability to detect bugs. The trick is to balance the needs of bug finding and speed so that a good commit build is stable enough for other people to work on.

Once the commit build is good then other people can work on the code with confidence. However there are further, slower, tests that we can start to do. Additional machines can run further testing routines on the build that take longer to do.

A simple example of this is a two stage deployment pipeline. The first stage would do the compilation and run tests that are more localized unit tests with slow services replaced by Test Doubles, such as a fake in-memory database or a stub for an external service. Such tests can run very fast, keeping within the ten minute guideline. However any bugs that involve larger scale interactions, particularly those involving the real database, won't be found. The second stage build runs a different suite of tests that do hit a real database and involve more end-to-end behavior. This suite might take a couple of hours to run.

In this scenario people use the first stage as the commit build and use this as their main CI cycle. If the secondary build fails, then this may not have the same 'stop everything' quality, but the team does aim to fix such bugs as rapidly as possible, while keeping the commit build running. Since the secondary build may be much slower, it may not run after every commit. In that case it runs as often as it can, picking the last good build from the commit stage.

If the secondary build detects a bug, that's a sign that the commit build could do with another test. As much as possible we want to ensure that any later-stage failure leads to new tests in the commit build that would have caught the bug, so the bug stays fixed in the commit build. This way the commit tests are strengthened whenever something gets past them. There are cases where there's no way to build a fast-running test that exposes the bug, so we may decide to only test for that condition in the secondary build. Most of the time, fortunately, we can add suitable tests to the commit build.

Another way to speed things up is to use parallelism and multiple machines. Cloud environments, in particular, allow teams to easily spin up a small fleet of servers for builds. Providing the tests can run reasonably independently, which well-written tests can, then using such a fleet can get very rapid build times. Such parallel cloud builds may also be worthwhile to a developer's pre-integration build too.

While we're considering the broader build process, it's worth mentioning another category of automation, interaction with dependencies. Most software uses a large range of dependent software produced by different organizations. Changes in these dependencies can cause breakages in the product. A team should thus automatically check for new versions of dependencies and integrate them into the build, essentially as if they were another team member. This should be done frequently, usually at least daily, depending on the rate of change of the dependencies. A similar approach should be used with running Contract Tests. If these dependency interactions go red, they don't have the same “stop the line” effect as regular build failures, but do require prompt action by the team to investigate and fix.

Hide Work-in-Progress

Continuous Integration means integrating as soon as there is a little forward progress and the build is healthy. Frequently this suggests integrating before a user-visible feature is fully formed and ready for release. We thus need to consider how to deal with latent code: code that's part of an unfinished feature that's present in a live release.

Some people worry about latent code, because it's putting non-production quality code into the released executable. Teams doing Continuous Integration ensure that all code sent to the mainline is production quality, together with the tests that verify the code. Latent code may never be executed in production, but that doesn't stop it from being exercised in tests.

We can prevent the code being executed in production by using a Keystone Interface - ensuring the interface that provides a path to the new feature is the last thing we add to the code base. Tests can still check the code at all levels other than that final interface. In a well-designed system, such interface elements should be minimal and thus simple to add with a short programming episode.

Using Dark Launching we can test some changes in production before we make them visible to the user. This technique is useful for assessing the impact on performance,

Keystones cover most cases of latent code, but for occasions where that's not possible we use Feature Flags. Feature flags are checked whenever we are about to execute latent code, they are set as part of the environment, perhaps in an environment-specific configuration file. That way the latent code can be active for testing, but disabled in production. As well as enabling Continuous Integration, feature flags also make it easier for runtime switching for A/B testing and Canary Releases. We then make sure we remove this logic promptly once a feature is fully released, so that the flags don't clutter the code base.

Branch By Abstraction is another technique for managing latent code, which is particularly useful for large infrastructural changes within a code base. Essentially this creates an internal interface to the modules that are being changed. The interface can then route between old and new logic, gradually replacing execution paths over time. We've seen this done to switch such pervasive elements as changing the persistence platform.

When introducing a new feature, we should always ensure that we can rollback in case of problems. Parallel Change (aka expand-contract) breaks a change into reversible steps. For example, if we rename a database field, we first create a new field with the new name, then write to both old and new fields, then copy data from the exisitng old fields, then read from the new field, and only then remove the old field. We can reverse any of these steps, which would not be possible if we made such a change all at once. Teams using Continuous Integration often look to break up changes in this way, keeping changes small and easy to undo.

Test in a Clone of the Production Environment

The point of testing is to flush out, under controlled conditions, any problem that the system will have in production. A significant part of this is the environment within which the production system will run. If we test in a different environment, every difference results in a risk that what happens under test won't happen in production.

Consequently, we want to set up our test environment to be as exact a mimic of our production environment as possible. Use the same database software, with the same versions, use the same version of the operating system. Put all the appropriate libraries that are in the production environment into the test environment, even if the system doesn't actually use them. Use the same IP addresses and ports, run it on the same hardware.

Virtual environments make it much easier than it was in the past to do this. We run production software in containers, and reliably build exactly the same containers for testing, even in a developer's workspace. It's worth the effort and cost to do this, the price is usually small compared to hunting down a single bug that crawled out of the hole created by environment mismatches.

Some software is designed to run in multiple environments, such as different operating systems and platform versions. The deployment pipeline should arrange for testing in all of these environments in parallel.

A point to take care of is when the production environment isn't as good as the development environment. Will the production software be running on machines connected with dodgy wifi, like smartphones? Then ensure a test environment mimics poor network connections.

Everyone can see what's happening

Continuous Integration is all about communication, so we want to ensure that everyone can easily see the state of the system and the changes that have been made to it.

One of the most important things to communicate is the state of the mainline build. CI Services have dashboards that allow everyone to see the state of any builds they are running. Often they link with other tools to broadcast build information to internal social media tools such as Slack. IDEs often have hooks into these mechanisms, so developers can be alerted while still inside the tool they are using for much of their work. Many teams only send out notifications for build failures, but I think it's worth sending out messages on success too. That way people get used to the regular signals and get a sense for the length of the build. Not to mention the fact that it's nice to get a “well done” every day, even if it's only from a CI server.

Teams that share a physical space often have some kind of always-on physical display for the build. Usually this takes the form of a large screen showing a simplified dashboard. This is particularly valuable to alert everyone to a broken build, often using the red/green colors on the mainline commit build.

One of the older physical displays I rather liked were the use of red and green lava lamps. One of the features of a lava lamp is that after they are turned on for a while they start to bubble. The idea was that if the red lamp came on, the team should fix the build before it starts to bubble. Physical displays for build status often got playful, adding some quirky personality to a team's workspace. I have fond memories of a dancing rabbit.

As well as the current state of the build, these displays can show useful information about recent history, which can be an indicator of project health. Back at the turn of the century I worked with a team who had a history of being unable to create stable builds. We put a calendar on the wall that showed a full year with a small square for each day. Every day the QA group would put a green sticker on the day if they had received one stable build that passed the commit tests, otherwise a red square. Over time the calendar revealed the state of the build process showing a steady improvement until green squares were so common that the calendar disappeared - its purpose fulfilled.

Automate Deployment

To do Continuous Integration we need multiple environments, one to run commit tests, probably more to run further parts of the deployment pipeline. Since we are moving executables between these environments multiple times a day, we'll want to do this automatically. So it's important to have scripts that will allow us to deploy the application into any environment easily.

With modern tools for virtualization, containerization, and serverless we can go further. Not just have scripts to deploy the product, but also scripts to build the required environment from scratch. This way we can start with a bare-bones environment that's available off-the-shelf, create the environment we need for the product to run, install the product, and run it - all entirely automatically. If we're using feature flags to hide work-in-progress, then these environments can be set up with all the feature-flags on, so these features can be tested with all immanent interactions.

A natural consequence of this is that these same scripts allow us to deploy into production with similar ease. Many teams deploy new code into production multiple times a day using these automations, but even if we choose a less frequent cadence, automatic deployment helps speed up the process and reduces errors. It's also a cheap option since it just uses the same capabilities that we use to deploy into test environments.

If we deploy into production automatically, one extra capability we find handy is automated rollback. Bad things do happen from time to time, and if smelly brown substances hit rotating metal, it's good to be able to quickly go back to the last known good state. Being able to automatically revert also reduces a lot of the tension of deployment, encouraging people to deploy more frequently and thus get new features out to users quickly. Blue Green Deployment allows us to both make new versions live quickly, and to roll back equally quickly if needed, by shifting traffic between deployed versions.

Automated Deployment make it easier to set up Canary Releases, deploying a new version of a product to a subset of our users in order to flush out problems before releasing to the full population.

Mobile applications are good examples of where it's essential to automate deployment into test environments, in this case onto devices so that a new version can be explored before invoking the guardians of the App Store. Indeed any device-bound software needs ways to easily get new versions on to test devices.

When deploying software like this, remember to ensure that version information is visible. An about screen should contain a build id that ties back to version control, logs should make it easy to see which version of the software is running, there should be some API endpoint that will give version information.

Benefits of Continuous Integration

When discussing the relative merits of the three styles of integration, most of the discussion is truly about the frequency of integration. Both Pre-Release Integration and Feature Branching can operate at different frequencies and it's possible to change integration frequency without changing the style of integration. If we're using Pre-Release Integration, there's a big difference between monthly releases and annual releases. Feature Branching usually works at a higher frequency, because integration occurs when each feature is individually pushed to mainline, as opposed to waiting to batch a bunch of units together. If a team is doing Feature Branching and all its features are less than a day's work to build, then they are effectively the same as Continuous Integration. But Continuous Integration is different in that it's defined as a high-frequency style. Continuous Integration makes a point of setting integration frequency as a target in itself, and not binding it to feature completion or release frequency.

It thus follows that most teams can see a useful improvement in the factors I'll discuss below by increasing their frequency without changing their style. There are significant benefits to reducing the size of features from two months to two weeks. Continuous Integration has the advantage of setting high-frequency integration as the baseline, setting habits and practices that make it sustainable.

Reduced risk of delivery delays

It's very hard to estimate how long it takes to do a complex integration. Sometimes it can be a struggle to merge in git, but then all works well. Other times it can be a quick merge, but a subtle integration bug takes days to find and fix. The longer the time between integrations, the more code to integrate, the longer it takes - but what's worse is the increase in unpredictability.

This all makes pre-release integration a special form of nightmare. Because the integration is one of the last steps before release, time is already tight and the pressure is on. Having a hard-to-predict phase late in the day means we have a significant risk that's very difficult to mitigate. That was why my 80's memory is so strong, and it's hardly the only time I've seen projects stuck in an integration hell, where every time they fix an integration bug, two more pop up.

Any steps to increase integration frequency lowers this risk. The less integration there is to do, the less unknown time there is before a new release is ready. Feature Branching helps by pushing this integration work to individual feature streams, so that, if left alone, a stream can push to mainline as soon as the feature is ready.

But that left alone point is important. If anyone else pushes to mainline, then we introduce some integration work before the feature is done. Because the branches are isolated, a developer working on one branch doesn't have much visibility about what other features may push, and how much work would be involved to integrate them. While there is a danger that high priority features can face integration delays, we can manage this by preventing pushes of lower-priority features.

Continuous Integration effectively eliminates delivery risk. The integrations are so small that they usually proceed without comment. An awkward integration would be one that takes more than a few minutes to resolve. The very worst case would be conflict that causes someone to restart their work from scratch, but that would still be less than a day's work to lose, and is thus not going to be something that's likely to trouble a board of stakeholders. Furthermore we're doing integration regularly as we develop the software, so we can face problems while we have more time to deal with them and can practice how to resolve them.

Even if a team isn't releasing to production regularly, Continuous Integration is important because it allows everyone to see exactly what the state of the product is. There's no hidden integration efforts that need to be done before release, any effort in integration is already baked in.

Less time wasted in integration

I've not seen any serious studies that measure how time spent on integration matches the size of integrations, but my anecdotal evidence strongly suggests that the relationship isn't linear. If there's twice as much code to integrate, it's more likely to be four times as long to carry out the integration. It's rather like how we need three lines to fully connect three nodes, but six lines to connect four of them. Integration is all about connections, hence the non-linear increase, one that's reflected in the experience of my colleagues.

In organizations that are using feature branches, much of this lost time is felt by the individual. Several hours spent trying to rebase on a big change to mainline is frustrating. A few days spent waiting for a code review on a finished pull request, which another big mainline change during the waiting period is even more frustrating. Having to put work on a new feature aside to debug a problem found in an integration test of feature finished two weeks ago saps productivity.

When we're doing Continuous Integration, integration is generally a non-event. I pull down the mainline, run the build, and push. If there is a conflict, the small amount of code I've written is fresh in my mind, so it's usually easy to see. The workflow is regular, so we're practiced at it, and we're incentives to automate it as much as possible.

Like many of these non-linear effects, integration can easily become a trap where people learn the wrong lesson. A difficult integration may be so traumatic that a team decides it should do integrations less often, which only exacerbates the problem in the future.

What's happening here is that we are seeing much closer collaboration between the members of the team. Should two developers make decisions that conflict, we find out when we integrate. So the less time between integrations, the less time before we detect the conflict, and we can deal with the conflict before it grows too big. With high-frequency integration, our source control system becomes a communication channel, one that can communicate things that can otherwise be unsaid.

Less Bugs

Bugs - these are the nasty things that destroy confidence and mess up schedules and reputations. Bugs in deployed software make users angry with us. Bugs cropping up during regular development get in our way, making it harder to get the rest of the software working correctly.

Continuous Integration doesn't get rid of bugs, but it does make them dramatically easier to find and remove. This is less because of the high-frequency integration and more due to the essential introduction of self-testing code. Continuous Integration doesn't work without self-testing code because without decent tests, we can't keep a healthy mainline. Continuous Integration thus institutes a regular regimen of testing. If the tests are inadequate, the team will quickly notice, and can take corrective action. If a bug appears due to a semantic conflict, it's easy to detect because there's only a small amount of code to be integrated. Frequent integrations also work well with Diff Debugging, so even a bug noticed weeks later can be narrowed down to a small change.

Bugs are also cumulative. The more bugs we have, the harder it is to remove each one. This is partly because we get bug interactions, where failures show as the result of multiple faults - making each fault harder to find. It's also psychological - people have less energy to find and get rid of bugs when there are many of them. Thus self-testing code reinforced by Continuous Integration has another exponential effect in reducing the problems caused by defects.

This runs into another phenomenon that many people find counter-intuitive. Seeing how often introducing a change means introducing bugs, people conclude that to have high reliability software they need to slow down the release rate. This was firmly contradicted by the DORA research program led by Nicole Forsgren. They found that elite teams deployed to production more rapidly, more frequently, and had a dramatically lower incidence of failure when they made these changes. The research also finds that teams have higher levels of performance when they have three or fewer active branches in the application’s code repository, merge branches to mainline at least once a day, and don’t have code freezes or integration phases.

Enables Refactoring for sustained productivity

Most teams observe that over time, codebases deteriorate. Early decisions were good at the time, but are no longer optimal after six month's work. But changing the code to incorporate what the team has learned means introducing changes deep in the existing code, which results in difficult merges which are both time-consuming and full of risk. Everyone recalls that time someone made what would be a good change for the future, but caused days of effort breaking other people's work. Given that experience, nobody wants to rework the structure of existing code, even though it's now awkward for everyone to build on, thus slowing down delivery of new features.

Refactoring is an essential technique to attenuate and indeed reverse this process of decay. A team that refactors regularly has a disciplined technique to improve the structure of a code base by using small, behavior-preserving transformations of the code. These characteristics of the transformations greatly reduce their chances of introducing bugs, and they can be done quickly, especially when supported by a foundation of self-testing code. Applying refactoring at every opportunity, a team can improve the structure of an existing codebase, making it easier and faster to add new capabilities.

But this happy story can be torpedoed by integration woes. A two week refactoring session may greatly improve the code, but result in long merges because everyone else has been spending the last two weeks working with the old structure. This raises the costs of refactoring to prohibitive levels. Frequent integration solves this dilemma by ensuring that both those doing the refactoring and everyone else are regularly synchronizing their work. When using Continuous Integration, if someone makes intrusive changes to a core library I'm using, I only have to adjust a few hours of programming to these changes. If they do something that clashes with the direction of my changes, I know right away, so have the opportunity to talk to them so we can figure out a better way forward.

So far in this article I've raised several counter-intuitive notions about the merits of high-frequency integration: that the more often we integrate, the less time we spend integrating, and that frequent integration leads to less bugs. Here is perhaps the most important counter-intuitive notion in software development: that teams that spend a lot of effort keeping their code base healthy deliver features faster and cheaper. Time invested in writing tests and refactoring delivers impressive returns in delivery speed, and Continuous Integration is a core part of making that work in a team setting.

Release to Production is a business decision

Imagine we are demonstrating some newly built feature to a stakeholder, and she reacts by saying - “this is really cool, and would make a big business impact. How long before we can make this live?” If that feature is being shown on an unintegrated branch, then the answer may be weeks or months, particularly if there is poor automation on the path to production. Continuous Integration allows us to maintain a Release-Ready Mainline, which means the decision to release the latest version of the product into production is purely a business decision. If the stakeholders want the latest to go live, it's a matter of minutes running an automated pipeline to make it so. This allows the customers of the software greater control of when features are released, and encourages them to collaborate more closely with the development team

Continuous Integration and a Release-Ready Mainline removes one of the biggest barriers to frequent deployment. Frequent deployment is valuable because it allows our users to get new features more rapidly, to give more rapid feedback on those features, and generally become more collaborative in the development cycle. This helps break down the barriers between customers and development - barriers which I believe are the biggest barriers to successful software development."
https://martinfowler.com/articles/microservices.html,Microservices,"Characteristics of a Microservice Architecture

We cannot say there is a formal definition of the microservices architectural style, but we can attempt to describe what we see as common characteristics for architectures that fit the label. As with any definition that outlines common characteristics, not all microservice architectures have all the characteristics, but we do expect that most microservice architectures exhibit most characteristics. While we authors have been active members of this rather loose community, our intention is to attempt a description of what we see in our own work and in similar efforts by teams we know of. In particular we are not laying down some definition to conform to.

Componentization via Services

For as long as we've been involved in the software industry, there's been a desire to build systems by plugging together components, much in the way we see things are made in the physical world. During the last couple of decades we've seen considerable progress with large compendiums of common libraries that are part of most language platforms.

When talking about components we run into the difficult definition of what makes a component. Our definition is that a component is a unit of software that is independently replaceable and upgradeable.

Microservice architectures will use libraries, but their primary way of componentizing their own software is by breaking down into services. We define libraries as components that are linked into a program and called using in-memory function calls, while services are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call. (This is a different concept to that of a service object in many OO programs .)

One main reason for using services as components (rather than libraries) is that services are independently deployable. If you have an application that consists of a multiple libraries in a single process, a change to any single component results in having to redeploy the entire application. But if that application is decomposed into multiple services, you can expect many single service changes to only require that service to be redeployed. That's not an absolute, some changes will change service interfaces resulting in some coordination, but the aim of a good microservice architecture is to minimize these through cohesive service boundaries and evolution mechanisms in the service contracts.

Another consequence of using services as components is a more explicit component interface. Most languages do not have a good mechanism for defining an explicit Published Interface. Often it's only documentation and discipline that prevents clients breaking a component's encapsulation, leading to overly-tight coupling between components. Services make it easier to avoid this by using explicit remote call mechanisms.

Using services like this does have downsides. Remote calls are more expensive than in-process calls, and thus remote APIs need to be coarser-grained, which is often more awkward to use. If you need to change the allocation of responsibilities between components, such movements of behavior are harder to do when you're crossing process boundaries.

At a first approximation, we can observe that services map to runtime processes, but that is only a first approximation. A service may consist of multiple processes that will always be developed and deployed together, such as an application process and a database that's only used by that service.

Organized around Business Capabilities

When looking to split a large application into parts, often management focuses on the technology layer, leading to UI teams, server-side logic teams, and database teams. When teams are separated along these lines, even simple changes can lead to a cross-team project taking time and budgetary approval. A smart team will optimise around this and plump for the lesser of two evils - just force the logic into whichever application they have access to. Logic everywhere in other words. This is an example of Conway's Law in action.

Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.

-- Melvin Conway, 1968

Figure 2: Conway's Law in action

The microservice approach to division is different, splitting up into services organized around business capability. Such services take a broad-stack implementation of software for that business area, including user-interface, persistant storage, and any external collaborations. Consequently the teams are cross-functional, including the full range of skills required for the development: user-experience, database, and project management.

Figure 3: Service boundaries reinforced by team boundaries

One company organised in this way is www.comparethemarket.com. Cross functional teams are responsible for building and operating each product and each product is split out into a number of individual services communicating via a message bus.

Large monolithic applications can always be modularized around business capabilities too, although that's not the common case. Certainly we would urge a large team building a monolithic application to divide itself along business lines. The main issue we have seen here, is that they tend to be organised around too many contexts. If the monolith spans many of these modular boundaries it can be difficult for individual members of a team to fit them into their short-term memory. Additionally we see that the modular lines require a great deal of discipline to enforce. The necessarily more explicit separation required by service components makes it easier to keep the team boundaries clear.

Products not Projects

Most application development efforts that we see use a project model: where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded.

Microservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime. A common inspiration for this is Amazon's notion of “you build, you run it” where a development team takes full responsibility for the software in production. This brings developers into day-to-day contact with how their software behaves in production and increases contact with their users, as they have to take on at least some of the support burden.

The product mentality, ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed, there is an on-going relationship where the question is how can software assist its users to enhance the business capability.

There's no reason why this same approach can't be taken with monolithic applications, but the smaller granularity of services can make it easier to create the personal relationships between service developers and their users.

Smart endpoints and dumb pipes

When building communication structures between different processes, we've seen many products and approaches that stress putting significant smarts into the communication mechanism itself. A good example of this is the Enterprise Service Bus (ESB), where ESB products often include sophisticated facilities for message routing, choreography, transformation, and applying business rules.

The microservice community favours an alternative approach: smart endpoints and dumb pipes. Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their own domain logic and act more as filters in the classical Unix sense - receiving a request, applying logic as appropriate and producing a response. These are choreographed using simple RESTish protocols rather than complex protocols such as WS-Choreography or BPEL or orchestration by a central tool.

The two protocols used most commonly are HTTP request-response with resource API's and lightweight messaging . The best expression of the first is

Be of the web, not behind the web

-- Ian Robinson

Microservice teams use the principles and protocols that the world wide web (and to a large extent, Unix) is built on. Often used resources can be cached with very little effort on the part of developers or operations folk.

The second approach in common use is messaging over a lightweight message bus. The infrastructure chosen is typically dumb (dumb as in acts as a message router only) - simple implementations such as RabbitMQ or ZeroMQ don't do much more than provide a reliable asynchronous fabric - the smarts still live in the end points that are producing and consuming messages; in the services.

In a monolith, the components are executing in-process and communication between them is via either method invocation or function call. The biggest issue in changing a monolith into microservices lies in changing the communication pattern. A naive conversion from in-memory method calls to RPC leads to chatty communications which don't perform well. Instead you need to replace the fine-grained communication with a coarser -grained approach.

Decentralized Governance

One of the consequences of centralised governance is the tendency to standardise on single technology platforms. Experience shows that this approach is constricting - not every problem is a nail and not every solution a hammer. We prefer using the right tool for the job and while monolithic applications can take advantage of different languages to a certain extent, it isn't that common.

Splitting the monolith's components out into services we have a choice when building each of them. You want to use Node.js to standup a simple reports page? Go for it. C++ for a particularly gnarly near-real-time component? Fine. You want to swap in a different flavour of database that better suits the read behaviour of one component? We have the technology to rebuild him.

Of course, just because you can do something, doesn't mean you should - but partitioning your system in this way means you have the option.

Teams building microservices prefer a different approach to standards too. Rather than use a set of defined standards written down somewhere on paper they prefer the idea of producing useful tools that other developers can use to solve similar problems to the ones they are facing. These tools are usually harvested from implementations and shared with a wider group, sometimes, but not exclusively using an internal open source model. Now that git and github have become the de facto version control system of choice, open source practices are becoming more and more common in-house .

Netflix is a good example of an organisation that follows this philosophy. Sharing useful and, above all, battle-tested code as libraries encourages other developers to solve similar problems in similar ways yet leaves the door open to picking a different approach if required. Shared libraries tend to be focused on common problems of data storage, inter-process communication and as we discuss further below, infrastructure automation.

For the microservice community, overheads are particularly unattractive. That isn't to say that the community doesn't value service contracts. Quite the opposite, since there tend to be many more of them. It's just that they are looking at different ways of managing those contracts. Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. These aid service contracts in evolving independently. Executing consumer driven contracts as part of your build increases confidence and provides fast feedback on whether your services are functioning. Indeed we know of a team in Australia who drive the build of new services with consumer driven contracts. They use simple tools that allow them to define the contract for a service. This becomes part of the automated build before code for the new service is even written. The service is then built out only to the point where it satisfies the contract - an elegant approach to avoid the 'YAGNI' dilemma when building new software. These techniques and the tooling growing up around them, limit the need for central contract management by decreasing the temporal coupling between services.

Perhaps the apogee of decentralised governance is the build it / run it ethos popularised by Amazon. Teams are responsible for all aspects of the software they build including operating the software 24/7. Devolution of this level of responsibility is definitely not the norm but we do see more and more companies pushing responsibility to the development teams. Netflix is another organisation that has adopted this ethos . Being woken up at 3am every night by your pager is certainly a powerful incentive to focus on quality when writing your code. These ideas are about as far away from the traditional centralized governance model as it is possible to be.

Decentralized Data Management

Decentralization of data management presents in a number of different ways. At the most abstract level, it means that the conceptual model of the world will differ between systems. This is a common issue when integrating across a large enterprise, the sales view of a customer will differ from the support view. Some things that are called customers in the sales view may not appear at all in the support view. Those that do may have different attributes and (worse) common attributes with subtly different semantics.

This issue is common between applications, but can also occur within applications, particular when that application is divided into separate components. A useful way of thinking about this is the Domain-Driven Design notion of Bounded Context. DDD divides a complex domain up into multiple bounded contexts and maps out the relationships between them. This process is useful for both monolithic and microservice architectures, but there is a natural correlation between service and context boundaries that helps clarify, and as we describe in the section on business capabilities, reinforce the separations.

As well as decentralizing decisions about conceptual models, microservices also decentralize data storage decisions. While monolithic applications prefer a single logical database for persistant data, enterprises often prefer a single database across a range of applications - many of these decisions driven through vendor's commercial models around licensing. Microservices prefer letting each service manage its own database, either different instances of the same database technology, or entirely different database systems - an approach called Polyglot Persistence. You can use polyglot persistence in a monolith, but it appears more frequently with microservices.

Decentralizing responsibility for data across microservices has implications for managing updates. The common approach to dealing with updates has been to use transactions to guarantee consistency when updating multiple resources. This approach is often used within monoliths.

Using transactions like this helps with consistency, but imposes significant temporal coupling, which is problematic across multiple services. Distributed transactions are notoriously difficult to implement and as a consequence microservice architectures emphasize transactionless coordination between services, with explicit recognition that consistency may only be eventual consistency and problems are dealt with by compensating operations.

Choosing to manage inconsistencies in this way is a new challenge for many development teams, but it is one that often matches business practice. Often businesses handle a degree of inconsistency in order to respond quickly to demand, while having some kind of reversal process to deal with mistakes. The trade-off is worth it as long as the cost of fixing mistakes is less than the cost of lost business under greater consistency.

Infrastructure Automation

Infrastructure automation techniques have evolved enormously over the last few years - the evolution of the cloud and AWS in particular has reduced the operational complexity of building, deploying and operating microservices.

Many of the products or systems being build with microservices are being built by teams with extensive experience of Continuous Delivery and it's precursor, Continuous Integration. Teams building software this way make extensive use of infrastructure automation techniques. This is illustrated in the build pipeline shown below.

Figure 5: basic build pipeline

Since this isn't an article on Continuous Delivery we will call attention to just a couple of key features here. We want as much confidence as possible that our software is working, so we run lots of automated tests. Promotion of working software 'up' the pipeline means we automate deployment to each new environment.

A monolithic application will be built, tested and pushed through these environments quite happlily. It turns out that once you have invested in automating the path to production for a monolith, then deploying more applications doesn't seem so scary any more. Remember, one of the aims of CD is to make deployment boring, so whether its one or three applications, as long as its still boring it doesn't matter .

Another area where we see teams using extensive infrastructure automation is when managing microservices in production. In contrast to our assertion above that as long as deployment is boring there isn't that much difference between monoliths and microservices, the operational landscape for each can be strikingly different.

Figure 6: Module deployment often differs

Design for failure

A consequence of using services as components, is that applications need to be designed so that they can tolerate the failure of services. Any service call could fail due to unavailability of the supplier, the client has to respond to this as gracefully as possible. This is a disadvantage compared to a monolithic design as it introduces additional complexity to handle it. The consequence is that microservice teams constantly reflect on how service failures affect the user experience. Netflix's Simian Army induces failures of services and even datacenters during the working day to test both the application's resilience and monitoring.

This kind of automated testing in production would be enough to give most operation groups the kind of shivers usually preceding a week off work. This isn't to say that monolithic architectural styles aren't capable of sophisticated monitoring setups - it's just less common in our experience.

Since services can fail at any time, it's important to be able to detect the failures quickly and, if possible, automatically restore service. Microservice applications put a lot of emphasis on real-time monitoring of the application, checking both architectural elements (how many requests per second is the database getting) and business relevant metrics (such as how many orders per minute are received). Semantic monitoring can provide an early warning system of something going wrong that triggers development teams to follow up and investigate.

This is particularly important to a microservices architecture because the microservice preference towards choreography and event collaboration leads to emergent behavior. While many pundits praise the value of serendipitous emergence, the truth is that emergent behavior can sometimes be a bad thing. Monitoring is vital to spot bad emergent behavior quickly so it can be fixed.

Monoliths can be built to be as transparent as a microservice - in fact, they should be. The difference is that you absolutely need to know when services running in different processes are disconnected. With libraries within the same process this kind of transparency is less likely to be useful.

Microservice teams would expect to see sophisticated monitoring and logging setups for each individual service such as dashboards showing up/down status and a variety of operational and business relevant metrics. Details on circuit breaker status, current throughput and latency are other examples we often encounter in the wild.

Evolutionary Design

Microservice practitioners, usually have come from an evolutionary design background and see service decomposition as a further tool to enable application developers to control changes in their application without slowing down change. Change control doesn't necessarily mean change reduction - with the right attitudes and tools you can make frequent, fast, and well-controlled changes to software.

Whenever you try to break a software system into components, you're faced with the decision of how to divide up the pieces - what are the principles on which we decide to slice up our application? The key property of a component is the notion of independent replacement and upgradeability - which implies we look for points where we can imagine rewriting a component without affecting its collaborators. Indeed many microservice groups take this further by explicitly expecting many services to be scrapped rather than evolved in the longer term.

The Guardian website is a good example of an application that was designed and built as a monolith, but has been evolving in a microservice direction. The monolith still is the core of the website, but they prefer to add new features by building microservices that use the monolith's API. This approach is particularly handy for features that are inherently temporary, such as specialized pages to handle a sporting event. Such a part of the website can quickly be put together using rapid development languages, and removed once the event is over. We've seen similar approaches at a financial institution where new services are added for a market opportunity and discarded after a few months or even weeks.

This emphasis on replaceability is a special case of a more general principle of modular design, which is to drive modularity through the pattern of change . You want to keep things that change at the same time in the same module. Parts of a system that change rarely should be in different services to those that are currently undergoing lots of churn. If you find yourself repeatedly changing two services together, that's a sign that they should be merged.

Putting components into services adds an opportunity for more granular release planning. With a monolith any changes require a full build and deployment of the entire application. With microservices, however, you only need to redeploy the service(s) you modified. This can simplify and speed up the release process. The downside is that you have to worry about changes to one service breaking its consumers. The traditional integration approach is to try to deal with this problem using versioning, but the preference in the microservice world is to only use versioning as a last resort. We can avoid a lot of versioning by designing services to be as tolerant as possible to changes in their suppliers.

Are Microservices the Future?

Our main aim in writing this article is to explain the major ideas and principles of microservices. By taking the time to do this we clearly think that the microservices architectural style is an important idea - one worth serious consideration for enterprise applications. We have recently built several systems using the style and know of others who have used and favor this approach.

Those we know about who are in some way pioneering the architectural style include Amazon, Netflix, The Guardian, the UK Government Digital Service, realestate.com.au, Forward and comparethemarket.com. The conference circuit in 2013 was full of examples of companies that are moving to something that would class as microservices - including Travis CI. In addition there are plenty of organizations that have long been doing what we would class as microservices, but without ever using the name. (Often this is labelled as SOA - although, as we've said, SOA comes in many contradictory forms. )

Despite these positive experiences, however, we aren't arguing that we are certain that microservices are the future direction for software architectures. While our experiences so far are positive compared to monolithic applications, we're conscious of the fact that not enough time has passed for us to make a full judgement.

Often the true consequences of your architectural decisions are only evident several years after you made them. We have seen projects where a good team, with a strong desire for modularity, has built a monolithic architecture that has decayed over the years. Many people believe that such decay is less likely with microservices, since the service boundaries are explicit and hard to patch around. Yet until we see enough systems with enough age, we can't truly assess how microservice architectures mature.

There are certainly reasons why one might expect microservices to mature poorly. In any effort at componentization, success depends on how well the software fits into components. It's hard to figure out exactly where the component boundaries should lie. Evolutionary design recognizes the difficulties of getting boundaries right and thus the importance of it being easy to refactor them. But when your components are services with remote communications, then refactoring is much harder than with in-process libraries. Moving code is difficult across service boundaries, any interface changes need to be coordinated between participants, layers of backwards compatibility need to be added, and testing is made more complicated.

Our colleague Sam Newman spent most of 2014 working on a book that captures our experiences with building microservices. This should be your next step if you want a deeper dive into the topic.

Another issue is If the components do not compose cleanly, then all you are doing is shifting complexity from inside a component to the connections between components. Not just does this just move complexity around, it moves it to a place that's less explicit and harder to control. It's easy to think things are better when you are looking at the inside of a small, simple component, while missing messy connections between services.

Finally, there is the factor of team skill. New techniques tend to be adopted by more skillful teams. But a technique that is more effective for a more skillful team isn't necessarily going to work for less skillful teams. We've seen plenty of cases of less skillful teams building messy monolithic architectures, but it takes time to see what happens when this kind of mess occurs with microservices. A poor team will always create a poor system - it's very hard to tell if microservices reduce the mess in this case or make it worse.

One reasonable argument we've heard is that you shouldn't start with a microservices architecture. Instead begin with a monolith, keep it modular, and split it into microservices once the monolith becomes a problem. (Although this advice isn't ideal, since a good in-process interface is usually not a good service interface.)

So we write this with cautious optimism. So far, we've seen enough about the microservice style to feel that it can be a worthwhile road to tread. We can't say for sure where we'll end up, but one of the challenges of software development is that you can only make decisions based on the imperfect information that you currently have to hand."
https://dropbox.tech/security/how-we-use-lakera-guard-to-secure-our-llms,How we use Lakera Guard to secure our LLMs,"From search to organization, rapid advancements in artificial intelligence (AI) have made it easier for Dropbox users to discover and interact with their files. However, these advancements can also introduce new security challenges. Large Language Models (LLMs), integral to some of our most recent intelligent features, are also susceptible to various threats—from data breaches and adversarial attacks to exploitation by malicious actors. While hundreds of millions of users already trust Dropbox to protect their content, ensuring the security and integrity of these models is essential for maintaining that trust.

Last year we evaluated several security solutions to help safeguard our LLM-powered applications and ultimately chose Lakera Guard. With its robust capabilities, Lakera Guard helps us secure and protect user data, and—as outlined in our AI principles—uphold the reliability and trustworthiness of our intelligent features.

Addressing these challenges requires a multifaceted approach, incorporating stringent security protocols, continuous monitoring, and proactive risk management strategies. In this story, we’ll share insights into our approach to securing our LLMs, the criteria we used to evaluate potential solutions, and the key benefits of implementing Lakera's technology.

What we were looking for

LLM security is comprised of many parts. Common problems include reliability, consistency, alignment, and adversarial attacks. However, the scope of the problem we were trying to solve was more customer-centric—specifically, using LLMs to chat about, summarize, transcribe, and retrieve information, in addition to agent/assistant use cases. These kinds of untrusted user inputs could result in moderation issues or prompt injection—a method sometimes used to manipulate models—which creates a lot of headaches, including undesirable model outputs.

We considered a variety of open source, in-house, and proprietary options before narrowing our criteria to either open source or commercial tools. Whatever we chose, we decided the following requirements were mandatory:

We couldn’t call out to a third party. The solution had to be deployable in-house on our existing infrastructure.

Low latency. Dropbox is committed to maximizing performance for users across all of its products. We couldn’t add latency to LLM-powered features any more than absolutely necessary, so we determined upper latency numbers with the product teams.

Latency for a given context length is also an important sub-problem here. Many options perform well on context lengths of <800 tokens, but drop off significantly at >4,000. Excellent support for long context lengths—the ability for models to process greater amounts of information—was critical, as many customer use cases routinely exceed this number.

Confidence scores. API integrations that not only allowed extensive control over the categories of blocking, but also the sensitivity, were key (eg., separating the danger classification jailbreak based on confidence scores in order to ensure we could meet the diverse needs of our product teams).

Future intelligence and continuous improvement. Since LLM security is a fast evolving space, we wanted a solution that could also give us actionable insights into attacks and payloads in a rapidly shifting environment.

In fact, given the rapidly shifting environment, our top priority was selecting a solution that gave us enough of a foothold to observe and reorient as needed.

How we tested

Once we had a short list of open-source and commercial tools that met our criteria, we set up each tool internally for evaluation. For our test suite, we used Garak, an open-source LLM vulnerability scanner customized to run Dropbox-specific security tests. With Garak, we could evaluate the security coverage of each of the potential solutions. This allowed us to conduct a range of tests involving prompt injection, jailbreak, and other security assessments developed by Dropbox.

We then tested each solution directly against a range of LLMs already in use or under evaluation by our product teams. This enabled us to establish a baseline of each model’s vulnerability. For example, if a security tool blocked 90% of malicious prompts, but the LLM had already mitigated 85% of these vulnerabilities, we measured a net improvement of only 5%.

Finally, we needed a tool that did not add excessive latency to LLM calls and respected the privacy of customer data (e.g., did not store prompt content or send it outside the Dropbox network). For this, we measured the response time of each security test and also monitored network requests and file changes to detect any potential breaches of user data.

After extensive testing, Lakera Guard emerged as the product meeting all our requirements, offering both the lowest latency and highest security coverage.

How we integrated Lakera Guard

Lakera provides a Docker container that we run as an internal service at Dropbox. This means Lakera Guard is just an RPC call away from any LLM pipeline. Conceptually, the LLM security architecture at Dropbox is designed using LangChain as shown in the figure below.

Here, a textual LLM prompt is directed through one or more prompt security chains before hitting the model. We have a security chain that makes Lakera Guard security API endpoint requests to our internally-hosted Docker container, which responds with confidence scores for prompt injection and jailbreak attacks. Dropbox services can then action on the returned Lakera Guard prompt security categories as appropriate for the application.

Prompts that are deemed to be safe are then passed to the LLM—either a third-party model, like GPT-4, or an internally hosted open-source model, like LLaMA 3, depending on the use case—which produces a textual response. The LLM’s response is then passed through our content moderation chains, which analyze the text for potentially harmful topics. The moderation chain calls out to Lakera’s content moderation API endpoint to identify harassing or explicit content that the Dropbox feature or service can withhold from the user as configured.

Integrating Lakera Guard into our Dropbox infrastructure was a gradual process. We started with one product directly calling the Lakera-provided Docker container. Eventually, we created a custom Dropbox service that can automatically scale up more containers as needed—and can be called via the LLM security layer we built as part of Dropbox’s central machine learning libraries.

What we learned, and what’s next

Throughout this process, several product teams had concerns about latency—especially since many LLM use cases at Dropbox have prompts of more than 8,000 characters. We worked closely with Lakera to minimize added latency as much as possible, and our current average latency is now a 7x improvement for prompts with more than 8,000 characters.

Our belief in Lakera is so strong that we've invested in its continued success and have collaborated with its teams on numerous improvements to Lakera Guard itself. We found novel ways to cause LLM hallucination and collaborated with Lakera to increase efficacy for malicious prompt detection. We also shared internal research—such as our work on repeated token attacks—as well as some interesting false positives.

Finally, by working closely with the machine learning and product teams, we were able to help them meet their requirements while also achieving our security goals. For example, some of the false positives we encountered were a result of poor user input sanitization—or the process of filtering any potentially harmful or unwanted characters—which we were able to pass back to the product teams for improvement. Lakera has also been very interested in understanding our product flows to ensure they’re delivering a product that meets us where we are.

We’re currently planning to expand our Lakera Guard integration to all products using LLMs at Dropbox. This will involve tuning the detections for each use case and determining other potential causes of false positives or high latencies that can occur with some of the different data structures currently in use.

One of our core commitments at Dropbox is being worthy of our customers' trust. Partnering with Lakera to protect our users and their data is a testament to this promise. There are also many more interesting problems yet to be solved, and we plan to share more about how our approach to LLM security continues to evolve in future posts.

~ ~ ~

If building innovative products, experiences, and infrastructure excites you, come build the future with us! Visit dropbox.com/jobs to see our open roles, and follow @LifeInsideDropbox on Instagram and Facebook to see what it's like to create a more enlightened way of working."
